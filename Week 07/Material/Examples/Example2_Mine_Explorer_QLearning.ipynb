{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57b305f",
   "metadata": {},
   "source": [
    "### Build your own environment ###\n",
    "\n",
    "Mining Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39fa1c-4cd4-4750-8e7f-b379f16eeecc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0fb00-edf9-4a9a-b296-0be1442d1677",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Build the MineExplorer environement\n",
    "\n",
    "class MineralExplorerEnv(gym.Env):\n",
    "    def __init__(self,render_mode = None):\n",
    "        super(MineralExplorerEnv, self).__init__()\n",
    "        \n",
    "        # Set the render mode for the environment\n",
    "        self.render_mode = render_mode  \n",
    "        # Action space: 0 = move left, 1 = move right\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Observation space: 6 discrete states (0 to 5)\n",
    "        self.observation_space = spaces.Discrete(6)\n",
    "        \n",
    "        # Initial state (Explorer starts in rectangle 3 -> index 2)\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Resets the environment to the initial state (state 3) and uses options if provided.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Use options if provided\n",
    "        if options is not None:\n",
    "            if 'start_state' in options:\n",
    "                self.state = options['start_state']\n",
    "            else:\n",
    "                self.state = random.randint(1,4)  # Default starting state selected randomly\n",
    "        else:\n",
    "            self.state = random.randint(1,4) # Default starting state selected randomly\n",
    "    \n",
    "        # Return observation (state) and an empty info dictionary\n",
    "        return self.state, {}\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Executes one time step within the environment\"\"\"\n",
    "\n",
    "        # Save the current state to calculate the reward based on the current state\n",
    "        #current_state = self.state\n",
    "        \n",
    "        # Move left (action=0) or right (action=1)\n",
    "        if action == 0:  # Move left\n",
    "            self.state = max(0, self.state - 1)  # Ensure we don't go below state 0\n",
    "        elif action == 1:  # Move right\n",
    "            self.state = min(5, self.state + 1)  # Ensure we don't go beyond state 5\n",
    "\n",
    "        # Check if the agent reached a terminal state (state 0 or state 5)\n",
    "        if self.state == 0:\n",
    "            reward = 0\n",
    "            done = True  # Terminal state reached, episode ends\n",
    "            truncated = False  # Not truncated by time limit\n",
    "        elif self.state == 5:\n",
    "            reward = 0\n",
    "            done = True  # Terminal state reached, episode ends\n",
    "            truncated = False  # Not truncated by time limit\n",
    "        else:\n",
    "            reward = 0\n",
    "            done = False  # Episode continues\n",
    "            truncated = False  # No truncation applied in this case\n",
    "        \n",
    "        # Return state (observation), scalar reward, done flag, truncated flag, and an empty info dictionary\n",
    "        return self.state, reward, done, truncated, {}\n",
    "\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"Render the environment\"\"\"\n",
    "        env_map = ['_' for _ in range(6)]\n",
    "        env_map[self.state] = 'E'\n",
    "        \n",
    "        if mode == \"human\":\n",
    "            print(f\"Environment: {' '.join(env_map)}\")\n",
    "        elif mode == \"ansi\":\n",
    "            return f\"Environment: {' '.join(env_map)}\"\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Optional cleanup method.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb06c1-5687-4a18-9485-6c6ca00990cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Testing the environment\n",
    "env = MineralExplorerEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dffe48-880c-419b-bb86-aa24fc67cf5b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "state, _ = env.reset()\n",
    "print(state)\n",
    "env.render()\n",
    "\n",
    "# Run an episode with random actions\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Random action (0: left, 1: right)\n",
    "    state, reward, done, info, _ = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Action: {action}, Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc1d919-657d-4624-80af-01066bd6c9ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.6  # Exploration rate\n",
    "epsilon_decay = 0.99  # Decay for epsilon after each episode\n",
    "min_epsilon = 0.01  # Minimum exploration rate\n",
    "episodes = 500000 # Number of episodes\n",
    "\n",
    "# Initialize the Q-Table with appropriate rewards for terminal states\n",
    "q_table = np.zeros((6, 2))\n",
    "q_table[0, 0] = 100  # State 0, left action (no action should give this state a reward of 100)\n",
    "q_table[5, 1] = 40   # State 5, right action (no action should give this state a reward of 40)\n",
    "\n",
    "# Initialize the environment\n",
    "env = MineralExplorerEnv()\n",
    "\n",
    "\n",
    "# Q-Learning algorithm\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()  # Start with a random initial state\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Choose action (epsilon-greedy strategy)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore: random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Exploit: choose the action with the highest Q-value\n",
    "\n",
    "        # Take the action and observe the result\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        #print(state,reward,next_state)\n",
    "\n",
    "        # Update Q-value for the current state-action pair (regardless of terminal or not)\n",
    "        q_table[state, action] = q_table[state, action] + alpha * (\n",
    "            reward + gamma * np.max(q_table[next_state]) - q_table[state, action]\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Transition to the next state\n",
    "        state = next_state\n",
    "\n",
    "    # Decay epsilon to reduce exploration over time\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "# Set the Q-values for invalid actions in terminal states to 0\n",
    "q_table[0, 1] = 0  # Can't move right from state 0\n",
    "q_table[5, 0] = 0  # Can't move left from state 5\n",
    "\n",
    "# Print the learned Q-Table\n",
    "print(\"Learned Q-Table:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741d978-52af-44b0-a592-d59b34145893",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Testing the learned policy\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "env.render()\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6026292c-d301-46a3-bc4a-8efcee6866e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usg_gym",
   "language": "python",
   "name": "usg_gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
