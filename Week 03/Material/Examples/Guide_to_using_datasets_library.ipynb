{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df45eed8-a800-4362-8df8-5e720734a7ee",
   "metadata": {},
   "source": [
    "### Tutorial on using the HuggingFace 'datasets' library\n",
    "\n",
    "This script provides a solid foundation for using the datasets library in your NLP projects. It shows you how to load, inspect, and transform data, as well as how to integrate it with pandas, which is a common workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5213f2-cfa6-445d-bf07-75ea5469d855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raoji\\miniconda3\\envs\\py312_torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# This tutorial demonstrates key functionalities of the Hugging Face `datasets` library.\n",
    "# It is designed for those building NLP applications with the `transformers` library.\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f88d1e9e-f6c3-4f3d-add2-9c5f5ea380a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Loading the 'imdb' dataset from the Hugging Face Hub.\n",
      "\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "\n",
      "Training dataset features: {'text': Value('string'), 'label': ClassLabel(names=['neg', 'pos'])}\n",
      "Number of rows in training dataset: 25000\n",
      "\n",
      "First example in the training dataset:\n",
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# Loading a Dataset from the Hugging Face Hub\n",
    "# The easiest way to get started is to load a pre-existing dataset.\n",
    "# The `load_dataset` function automatically downloads and caches the data.\n",
    "print(\"Step 2: Loading the 'imdb' dataset from the Hugging Face Hub.\")\n",
    "raw_datasets = load_dataset(\"imdb\")\n",
    "\n",
    "# Check the structure of the loaded dataset. It's a `DatasetDict` object.\n",
    "print(\"\\nDataset structure:\")\n",
    "print(raw_datasets)\n",
    "\n",
    "# Access a specific split, like 'train'\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "\n",
    "# Print the features (columns) and the number of rows\n",
    "print(f\"\\nTraining dataset features: {train_dataset.features}\")\n",
    "print(f\"Number of rows in training dataset: {len(train_dataset)}\")\n",
    "\n",
    "# Inspect a single example\n",
    "print(\"\\nFirst example in the training dataset:\")\n",
    "print(train_dataset[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6e6eaa-3000-4561-8722-71f9f19b4dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Converting a Hugging Face Dataset to a Pandas DataFrame.\n",
      "\n",
      "Pandas DataFrame head:\n",
      "                                                text  label\n",
      "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
      "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
      "2  If only to avoid making this type of film in t...      0\n",
      "3  This film was probably inspired by Godard's Ma...      0\n",
      "4  Oh, brother...after hearing about this ridicul...      0\n",
      "\n",
      "Creating a Hugging Face Dataset from the Pandas DataFrame.\n",
      "\n",
      "New Dataset created from Pandas DataFrame:\n",
      "Dataset({\n",
      "    features: ['text', 'label', '__index_level_0__'],\n",
      "    num_rows: 100\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Converting a Dataset to and from a Pandas DataFrame\n",
    "# The `datasets` library integrates seamlessly with pandas.\n",
    "\n",
    "# To Pandas DataFrame\n",
    "print(\"\\nStep 3: Converting a Hugging Face Dataset to a Pandas DataFrame.\")\n",
    "df = train_dataset.to_pandas()\n",
    "print(\"\\nPandas DataFrame head:\")\n",
    "print(df.head())\n",
    "\n",
    "# From Pandas DataFrame\n",
    "print(\"\\nCreating a Hugging Face Dataset from the Pandas DataFrame.\")\n",
    "# You can convert the entire DataFrame or a portion of it.\n",
    "df_small = df.sample(100)\n",
    "new_dataset_from_pandas = Dataset.from_pandas(df_small)\n",
    "print(\"\\nNew Dataset created from Pandas DataFrame:\")\n",
    "print(new_dataset_from_pandas)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24f31c9c-4223-4ada-a10c-6a1dfdb590bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Tokenizing the dataset for a transformer model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 50000/50000 [00:10<00:00, 4716.13 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized dataset features (note the new 'input_ids' and other keys):\n",
      "{'text': Value('string'), 'label': ClassLabel(names=['neg', 'pos']), 'input_ids': List(Value('int32')), 'token_type_ids': List(Value('int8')), 'attention_mask': List(Value('int8'))}\n",
      "\n",
      "First example of the tokenized dataset:\n",
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0, 'input_ids': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Data for a Transformer Model\n",
    "# This is a critical step for preparing your data for a model.\n",
    "# We'll use a tokenizer to convert text into numerical token IDs.\n",
    "\n",
    "# Load a pre-trained tokenizer. We'll use a small, efficient model for this example.\n",
    "print(\"\\nStep 4: Tokenizing the dataset for a transformer model.\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "# Create a preprocessing function that tokenizes a batch of examples.\n",
    "# The `batched=True` argument in `map` is crucial for efficiency.\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply the preprocessing function to the entire dataset\n",
    "# The `map` function is the core of data preprocessing in `datasets`.\n",
    "# It applies a function to all examples in the dataset and caches the result.\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"\\nTokenized dataset features (note the new 'input_ids' and other keys):\")\n",
    "print(tokenized_datasets[\"train\"].features)\n",
    "\n",
    "print(\"\\nFirst example of the tokenized dataset:\")\n",
    "print(tokenized_datasets[\"train\"][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a529bcf2-9862-4507-85cc-ed73b9a95f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Preparing the dataset for PyTorch training.\n",
      "\n",
      "Final prepared dataset structure:\n",
      "Column([tensor([  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,  2026,\n",
      "         2678,  3573,  2138,  1997,  2035,  1996,  6704,  2008,  5129,  2009,\n",
      "         2043,  2009,  2001,  2034,  2207,  1999,  3476,  1012,  1045,  2036,\n",
      "         2657,  2008,  2012,  2034,  2009,  2001,  8243,  2011,  1057,  1012,\n",
      "         1055,  1012,  8205,  2065,  2009,  2412,  2699,  2000,  4607,  2023,\n",
      "         2406,  1010,  3568,  2108,  1037,  5470,  1997,  3152,  2641,  1000,\n",
      "         6801,  1000,  1045,  2428,  2018,  2000,  2156,  2023,  2005,  2870,\n",
      "         1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,\n",
      "         5436,  2003,  8857,  2105,  1037,  2402,  4467,  3689,  3076,  2315,\n",
      "        14229,  2040,  4122,  2000,  4553,  2673,  2016,  2064,  2055,  2166,\n",
      "         1012,  1999,  3327,  2016,  4122,  2000,  3579,  2014,  3086,  2015,\n",
      "         2000,  2437,  2070,  4066,  1997,  4516,  2006,  2054,  1996,  2779,\n",
      "        25430, 14728,  2245,  2055,  3056,  2576,  3314,   102]), tensor([  101,  1000,  1045,  2572,  8025,  1024,  3756,  1000,  2003,  1037,\n",
      "        15544, 19307,  1998,  3653,  6528, 20771, 19986,  8632,  1012,  2009,\n",
      "         2987,  1005,  1056,  3043,  2054,  2028,  1005,  1055,  2576,  5328,\n",
      "         2024,  2138,  2023,  2143,  2064,  6684,  2022,  2579,  5667,  2006,\n",
      "         2151,  2504,  1012,  2004,  2005,  1996,  4366,  2008, 19124,  3287,\n",
      "        16371, 25469,  2003,  2019,  6882, 13316,  1011,  2459,  1010,  2008,\n",
      "         3475,  1005,  1056,  2995,  1012,  1045,  1005,  2310,  2464,  1054,\n",
      "         1011,  6758,  3152,  2007,  3287, 16371, 25469,  1012,  4379,  1010,\n",
      "         2027,  2069,  3749,  2070, 25085,  5328,  1010,  2021,  2073,  2024,\n",
      "         1996,  1054,  1011,  6758,  3152,  2007, 21226, 24728, 22144,  2015,\n",
      "         1998, 20916,  4691,  6845,  2401,  1029,  7880,  1010,  2138,  2027,\n",
      "         2123,  1005,  1056,  4839,  1012,  1996,  2168,  3632,  2005,  2216,\n",
      "        10231,  7685,  5830,  3065,  1024,  8040,  7317,   102]), tensor([  101,  2065,  2069,  2000,  4468,  2437,  2023,  2828,  1997,  2143,\n",
      "         1999,  1996,  2925,  1012,  2023,  2143,  2003,  5875,  2004,  2019,\n",
      "         7551,  2021,  4136,  2053,  2522, 11461,  2466,  1012,  1026,  7987,\n",
      "         1013,  1028,  1026,  7987,  1013,  1028,  2028,  2453,  2514,  6819,\n",
      "         5339,  8918,  2005,  3564, 27046,  2009,  2138,  2009, 12817,  2006,\n",
      "         2061,  2116,  2590,  3314,  2021,  2009,  2515,  2061,  2302,  2151,\n",
      "         5860, 11795,  3085, 15793,  1012,  1996, 13972,  3310,  2185,  2007,\n",
      "         2053,  2047, 15251,  1006,  4983,  2028,  3310,  2039,  2007,  2028,\n",
      "         2096,  2028,  1005,  1055,  2568, 17677,  2015,  1010,  2004,  2009,\n",
      "         2097, 26597,  2079,  2076,  2023, 23100,  2143,  1007,  1012,  1026,\n",
      "         7987,  1013,  1028,  1026,  7987,  1013,  1028,  2028,  2453,  2488,\n",
      "         5247,  2028,  1005,  1055,  2051,  4582,  2041,  1037,  3332,  2012,\n",
      "         1037,  3392,  3652,  1012,  1026,  7987,  1013,   102]), tensor([  101,  2023,  2143,  2001,  2763,  4427,  2011,  2643,  4232,  1005,\n",
      "         1055, 16137, 10841,  4115,  1010, 10768, 25300,  2078,  1998,  1045,\n",
      "         9075,  2017,  2000,  2156,  2008,  2143,  2612,  1012,  1026,  7987,\n",
      "         1013,  1028,  1026,  7987,  1013,  1028,  1996,  2143,  2038,  2048,\n",
      "         2844,  3787,  1998,  2216,  2024,  1010,  1006,  1015,  1007,  1996,\n",
      "        12689,  3772,  1006,  1016,  1007,  1996,  8052,  1010,  6151,  6810,\n",
      "         2099,  7178,  2135,  2204,  1010,  6302,  1012,  4237,  2013,  2008,\n",
      "         1010,  2054,  9326,  2033,  2087,  2003,  1996, 10866,  5460,  1997,\n",
      "         9033, 21202,  7971,  1012, 14229,  6396,  2386,  2038,  2000,  2022,\n",
      "         2087, 15703,  3883,  1999,  1996,  2088,  1012,  2016,  4490,  2061,\n",
      "         5236,  1998,  2007,  2035,  1996, 16371, 25469,  1999,  2023,  2143,\n",
      "         1010,  1012,  1012,  1012,  2009,  1005,  1055, 14477,  4779, 26884,\n",
      "         1012, 13599,  2000,  2643,  4232,  1005,  1055,   102]), tensor([  101,  2821,  1010,  2567,  1012,  1012,  1012,  2044,  4994,  2055,\n",
      "         2023,  9951,  2143,  2005,  8529, 13876, 12129,  2086,  2035,  1045,\n",
      "         2064,  2228,  1997,  2003,  2008,  2214, 14911,  3389,  2299,  1012,\n",
      "         1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1000,\n",
      "         2003,  2008,  2035,  2045,  2003,  1029,  1029,  1000,  1012,  1012,\n",
      "         1012,  1045,  2001,  2074,  2019,  2220,  9458,  2043,  2023, 20482,\n",
      "         3869,  2718,  1996,  1057,  1012,  1055,  1012,  1045,  2001,  2205,\n",
      "         2402,  2000,  2131,  1999,  1996,  4258,  1006,  2348,  1045,  2106,\n",
      "         6133,  2000, 13583,  2046,  1000,  9119,  8912,  1000,  1007,  1012,\n",
      "         2059,  1037, 11326,  2012,  1037,  2334,  2143,  2688, 10272, 17799,\n",
      "         1011,  2633,  1045,  2071,  2156,  2023,  2143,  1010,  3272,  2085,\n",
      "         1045,  2001,  2004,  2214,  2004,  2026,  3008,  2020,  2043,  2027,\n",
      "         8040,  7317, 13699,  5669,  2000,  2156,  2009,   102])])\n"
     ]
    }
   ],
   "source": [
    "# Preparing for Training\n",
    "# Select and rename columns, and set the format for your deep learning framework.\n",
    "\n",
    "# We'll assume a training setup where the model expects 'input_ids', 'attention_mask',\n",
    "# and 'labels' (not 'text' or 'label').\n",
    "print(\"\\nStep 5: Preparing the dataset for PyTorch training.\")\n",
    "\n",
    "# Remove original columns that the model doesn't need\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "# Rename the 'label' column to 'labels' as many models expect this name\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Set the format to 'torch' to return PyTorch tensors instead of standard Python lists\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "print(\"\\nFinal prepared dataset structure:\")\n",
    "print(tokenized_datasets[\"train\"]['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6901a6fa-0747-49ca-a039-35aed1214814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_torch",
   "language": "python",
   "name": "py312_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
