{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df386da8-1cf0-4e63-9436-7e56bd8b4607",
   "metadata": {},
   "source": [
    "### NLP Example using FineTuning\n",
    "\n",
    "Key characteristics of the SST-2 dataset\n",
    "\n",
    "    Source: The dataset is based on the Stanford Sentiment Treebank, a corpus of sentences from movie reviews.\n",
    "    Task: It is a binary classification task, meaning the goal is to classify each sentence into one of two categories: positive or negative sentiment. Neutral sentences from the original Treebank are discarded.\n",
    "    Input/Output:\n",
    "        Input: A single sentence from a movie review, such as \"The movie was a masterpiece\" or \"A very long movie, dull in stretches\".\n",
    "        Output: The corresponding sentiment label, either positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dd66ad-1965-4c72-8d15-393265af3a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e35f991-4e20-42b1-a78f-7a8d09be3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load small dataset (binary sentiment)\n",
    "ds = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266f30a-4efb-420e-a644-5344e1c6c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True)\n",
    "ds_enc = ds.map(tok, batched=True)\n",
    "print(ds_enc)\n",
    "\n",
    "# if available\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d175f0ba-c646-4e96-97de-6deae5e2891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model + metric\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric  = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": acc_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80514239-f3bc-46e6-91e6-520d1c65788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate DataCollator\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3ac82-4182-48a6-b11c-aad344477b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./bert-sst2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,      \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_enc[\"train\"].shuffle(seed=42).select(range(15000)),  # keep it lighter\n",
    "    eval_dataset=ds_enc[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4229220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train + evaluate\n",
    "trainer.train()\n",
    "eval_res = trainer.evaluate()\n",
    "print(eval_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82955c92-d027-43e4-9b16-8232eb8e9d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick predictions\n",
    "from transformers import pipeline, AutoTokenizer,AutoModelForSequenceClassification\n",
    "\n",
    "# Specify the task and the path to your fine-tuned model\n",
    "# The task must match the one the model was trained for (e.g., 'text-classification', 'summarization', 'ner').\n",
    "model_path = \"./bert-sst2/checkpoint-1876/\" \n",
    "task = \"text-classification\" # Example task\n",
    "\n",
    "# Define your custom label mapping (from step 1)\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "# Initialize the pipeline and if the model was trained with a specifc tokenizer use the same\n",
    "# Load the model with the custom id2label mapping\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, id2label=id2label)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Pipeline\n",
    "classifier = pipeline(task, model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef67c4-c2b5-40b4-9949-3d62fec9516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data = [\n",
    "    \"This movie was really good!\",\n",
    "    \"The product did not meet my expectations.\",\n",
    "    \"I'm not sure how I feel about this.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89184ea6-67e3-4216-a5e1-591a51c712b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions on the list\n",
    "results = classifier(custom_data)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Text: '{custom_data[i]}', Prediction: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba253d1-531b-46c5-9eea-6a9a5b6f6927",
   "metadata": {},
   "source": [
    "### Some key points ####\n",
    "\n",
    "#### <u>A. DataCollator's role:</u>\n",
    "\n",
    "1. The DataCollatorWithPadding function in Hugging Face Transformers is used for dynamic padding at the batch level, which is a key difference from how a tokenizer might handle padding.\n",
    "Here's why both are necessary:\n",
    "\n",
    "    - Tokenizer's role:\n",
    "    A tokenizer converts raw text into numerical input IDs and can generate an attention_mask indicating which tokens are real and which are padding. When you call tokenizer(...) with padding=True and truncation=True, it typically pads or truncates each individual sequence to a predefined max_length.\n",
    "    - Data Collator's role (Dynamic Padding):\n",
    "    The DataCollatorWithPadding takes a batch of already tokenized samples and dynamically pads them to the length of the longest sequence within that specific batch.\n",
    "\n",
    "This offers several advantages:\n",
    "1. Efficiency: It avoids padding all sequences to a fixed, potentially very long max_length if many sequences in a batch are short. This reduces redundant computations on padding tokens and saves memory.\n",
    "2. Flexibility: It allows for variable-length inputs without requiring a global max_length to be set beforehand, which might not be optimal for all datasets or tasks.\n",
    "3. Batching Requirement: Deep learning models typically require inputs of uniform shape for efficient matrix operations. The data collator ensures all sequences within a batch have the same length for this purpose. \n",
    "\n",
    "In essence, while the tokenizer prepares individual sequences, the data collator optimizes the batching process by applying dynamic padding and generating the corresponding attention mask at the batch level for efficient model training.\n",
    "\n",
    "\n",
    "#### <u>B. Compute Metrics role:</u>\n",
    "\n",
    "In Hugging Face (HF) training, the compute_metrics function is a user-defined callback that allows you to calculate and report custom evaluation metrics during the training loop. It is an optional but powerful feature of the Trainer class that provides deeper insights into your model's performance than just the default training loss. \n",
    "What compute_metrics does in training is,cinstead of providing a single loss value, the compute_metrics function enables you to track task-specific performance metrics, such as:\n",
    "\n",
    "    Accuracy for classification tasks\n",
    "    F1-score for classification with imbalanced data\n",
    "    ROUGE for text summarization\n",
    "    SQuAD F1 and Exact Match for question answering\n",
    "    Word Error Rate (WER) for automatic speech recognition \n",
    "\n",
    "Here is a breakdown of its role in the training process:\n",
    "\n",
    "    -> Receives predictions and labels: During evaluation, the Trainer passes the model's predictions and the true labels from the evaluation dataset to your compute_metrics function.\n",
    "    -> Performs calculations: Your function then processes these predictions and labels to calculate your desired metrics. This often involves converting the model's raw output (logits) into a more usable format, like class predictions or decoded text.\n",
    "    -> Returns results: The function returns a dictionary where the keys are the names of the metrics (e.g., 'accuracy') and the values are their computed scores. The Trainer then logs and reports these metrics.\n",
    "    -> Offers flexibility: While the ðŸ¤— evaluate library provides many standard metrics, compute_metrics allows you to define complex, custom-tailored metrics for your specific use case. \n",
    "\n",
    "How to instantiate compute_metrics\n",
    "You don't \"instantiate\" compute_metrics like a class, but rather define a function and pass it as an argument to the Trainer. The function must have a specific signature: it takes an EvalPrediction object as input and returns a dictionary of metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb87be-efd4-4159-8eee-6dbd94942ece",
   "metadata": {},
   "source": [
    "#### Label to id\n",
    "\n",
    "Interpretation: \n",
    "\n",
    "The id2label and label2id mappings are saved with the model's configuration. This allows the model to automatically translate the numeric output predictions into meaningful, human-readable labels during inference with a pipeline() or a saved model checkpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335ee71-2e6b-403c-a883-3fa06854f2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_torch",
   "language": "python",
   "name": "py312_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
