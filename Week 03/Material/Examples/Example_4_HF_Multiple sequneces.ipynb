{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5ce840-23d8-4077-979e-df50184f2b4f",
   "metadata": {},
   "source": [
    "#### Handling multiple sequences of different length ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6800c4-ed92-4529-9b80-c90788634594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71facc9-142a-4ac2-9193-112c019c68fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Models expect a batch of inputs\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "# This line will fail.\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836387d-e489-432d-b1a5-3217645d7cd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "The problem is that we sent a single sequence to the model, whereas ðŸ¤— Transformers models expect a batch of sequences by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e7acdd-e5cd-4deb-8d61-debbdf3a465a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try it again but this time add a dimension for batch\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a1aa48-d59e-4424-bd09-722d4ff5b620",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Batching is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e51012-98b1-400e-a278-739067475d72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Batch of two identical sequences\n",
    "\n",
    "batched_ids = [ids, ids]\n",
    "input_ids = torch.tensor(batched_ids)\n",
    "\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5fec2-d8cd-41d5-bf2e-d43ce836ea8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Padding the inputs ####\n",
    "\n",
    "The following list of lists cannot be converted to a tensor becasue they are of different lengths:\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]\n",
    "\n",
    "In order to work around this, weâ€™ll use padding to make our tensors have a rectangular shape. \n",
    "\n",
    "* Padding makes sure all our sentences have the same length by adding a special word called the padding token to the sentences with fewer values. \n",
    "* For example, if you have 10 sentences with 10 words and 1 sentence with 20 words, padding will ensure all the sentences have 20 words. \n",
    "* The padding token ID can be found in tokenizer.pad_token_id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb7926-3ab8-4bf5-9c4b-5b69e05875ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "# Without padding\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200],\n",
    "]        \n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca019e6-10db-4ce2-b323-d5ace6565086",
   "metadata": {},
   "source": [
    "In our example, the resulting tensor looks like this after padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905afbc2-7f3e-44b1-9db8-f5f5037acde9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# With padding\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566e6c8-62f2-4341-903e-7368549685b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Thereâ€™s something wrong with the logits in our batched predictions: the second row should be the same as the logits for the second sentence, but weâ€™ve got completely different values!\n",
    "\n",
    "This is because the key feature of Transformer models is attention layers that contextualize each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an __attention mask__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c201719-418b-4c77-8196-a4754f4e10b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Attention masks ####\n",
    "\n",
    "Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b233f1e5-c3b4-49ec-9ff7-43bf61dbbfb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a3b819-d18d-4b7e-8322-5485952b1d69",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Longer sequences ####\n",
    "\n",
    "With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem:\n",
    "\n",
    "   * Use a model with a longer supported sequence length.\n",
    "   * Truncate your sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1c89e-f6c9-4cf9-964f-3ba7b319ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = tokenizer('The model should not be used to intentionally create hostile or alienating \\\n",
    "environments for people. In addition, the model was not trained to be factual or true representations of people or events, \\\n",
    "and therefore using the model to generate such content is out-of-scope for the abilities of this model. For instance, for sentences like This film was \\\n",
    "filmed in COUNTRY, this binary classification model will give radically different probabilities for the positive label depending on the \\\n",
    "country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. \\\n",
    "In this colab, AurÃ©lien GÃ©ron made an interesting map plotting these probabilities for each country. can refer to Hydrofluoric acid (a dangerously \\\n",
    "corrosive chemical, the aqueous solution of hydrogen fluoride gas) and High Frequency (a range of radio frequencies known for long-distance communication, \\\n",
    "like with amateur radio). It can also be an abbreviation for the international charity Humanity First or a variety of other organizations and technical \\\n",
    "terms depending on context. A 5% to 9% hydrofluoric acid gel is also commonly used to etch all ceramic dental restorations to improve bonding.[6] For similar reasons, \\\n",
    "dilute hydrofluoric acid is a component of household rust stain remover, in car washes in \"wheel cleaner\" compounds, in ceramic and fabric rust inhibitors, \\\n",
    "and in water spot removers.[5][7] Because of its ability to dissolve iron oxides as well as silica-based contaminants, \\\n",
    "hydrofluoric acid is used in pre-commissioning boilers that produce high-pressure steam. Hydrofluoric acid is also useful \\\n",
    "for dissolving rock samples (usually powdered) prior to analysis. In similar manner, this acid is used in acid macerations \\\n",
    "to extract organic fossils from silicate rocks. Fossiliferous rock may be immersed directly into the acid, or a cellulose \\\n",
    "nitrate film may be applied (dissolved in amyl acetate), which adheres to the organic component and allows the rock to be dissolved around it. [6] For similar reasons, \\\n",
    "dilute hydrofluoric acid is a component of household rust stain remover, in car washes in \"wheel cleaner\" compounds, in ceramic and fabric rust inhibitors, \\\n",
    "and in water spot removers.[5][7] Because of its ability to dissolve iron oxides as well as silica-based contaminants, \\\n",
    "hydrofluoric acid is used in pre-commissioning boilers that produce high-pressure steam. Hydrofluoric acid is also useful \\\n",
    "for dissolving rock samples (usually powdered) prior to analysis. In similar manner, this acid is used in acid macerations \\\n",
    "to extract organic fossils from silicate rocks. Fossiliferous rock may be immersed directly into the acid, or a cellulose \\\n",
    "nitrate film may be applied (dissolved in amyl acetate), which adheres to the organic component and allows the rock to be dissolved around it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecde2f6-e8aa-4ba0-bf94-0d864ac85692",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of input_ids: {len(xx['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51efa6dc-bfea-4b03-9523-ed8d66a03c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx['input_ids'] = torch.tensor([xx['input_ids']])\n",
    "model(xx['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3755ce-7b3c-4c0f-a578-88067227172a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_torch",
   "language": "python",
   "name": "py312_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
