{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac3975e-7dbf-4fe4-969a-78ac3872915d",
   "metadata": {},
   "source": [
    "#### Same Digital Twin but ####\n",
    "\n",
    "- The input to the PID cpontroller is the T_set_point and the T_predicted by PINN\n",
    "- The output of the PID controller is the heat required to be pumped in to correct error\n",
    "- The PINN determines the new Temperature at the next time step and that becomes the new input to PID controller.\n",
    "\n",
    "So instead of the analytical Physics model, we now use the trained PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b05c09-afa4-4782-9c79-c7851cac05c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368f3c7d-a282-4dcf-9926-67c7cfa241ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# PINN MODEL DEFINITION ---\n",
    "\n",
    "class OvenPINN(nn.Module):\n",
    "    \"\"\"\n",
    "    Physics-Informed Neural Network for Oven Temperature Modeling.\n",
    "    The network models the temperature T as a function of time (t) and\n",
    "    one-dimensional space (x) within the oven.\n",
    "    Input: (t, x) normalized between 0 and 1.\n",
    "    Output: T (Temperature), normalized between 0 and 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_hidden=50, n_layers=4):\n",
    "        super(OvenPINN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Input layer (t, x) -> 2 inputs\n",
    "        self.input_layer = nn.Linear(2, n_hidden)\n",
    "\n",
    "        # Hidden layers\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(n_hidden, n_hidden) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Output layer (Temperature T) -> 1 output\n",
    "        self.output_layer = nn.Linear(n_hidden, 1)\n",
    "\n",
    "        # Activation function\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is a tensor of shape (N, 2) where x[:, 0] is time t and x[:, 1] is position x.\n",
    "        \"\"\"\n",
    "        out = self.activation(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            out = self.activation(layer(out))\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "\n",
    "# PHYSICS LOSS (Heat Equation) ---\n",
    "\n",
    "def pde_loss(model, t_interior, x_interior, alpha):\n",
    "    \"\"\"\n",
    "    Calculates the Physics Loss based on the 1D Heat Equation:\n",
    "    ∂T/∂t = α * ∂²T/∂x²\n",
    "    Loss = (∂T/∂t - α * ∂²T/∂x²)^2\n",
    "    \"\"\"\n",
    "    # Combine t and x into a single tensor for input\n",
    "    tx_interior = torch.cat([t_interior, x_interior], dim=1)\n",
    "\n",
    "    # Enable gradients for inputs required for derivative calculation\n",
    "    tx_interior.requires_grad_(True)\n",
    "\n",
    "    # 1. Forward pass: T(t, x)\n",
    "    T = model(tx_interior)\n",
    "\n",
    "    # 2. Calculate first derivative: ∂T/∂t\n",
    "    T_t = grad(T, tx_interior, torch.ones_like(T), create_graph=True)[0][:, 0:1]\n",
    "\n",
    "    # 3. Calculate first derivative: ∂T/∂x\n",
    "    T_x = grad(T, tx_interior, torch.ones_like(T), create_graph=True)[0][:, 1:2]\n",
    "\n",
    "    # 4. Calculate second derivative: ∂²T/∂x²\n",
    "    T_xx = grad(T_x, tx_interior, torch.ones_like(T_x), create_graph=True)[0][:, 1:2]\n",
    "\n",
    "    # 5. Residual (Physics Loss): R = ∂T/∂t - α * ∂²T/∂x²\n",
    "    # We want R to be close to 0\n",
    "    pde_residual = T_t - alpha * T_xx\n",
    "\n",
    "    # Mean Squared Error of the residual\n",
    "    loss_pde = torch.mean(pde_residual ** 2)\n",
    "    return loss_pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7934834",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CLASSIC PID CONTROLLER ---\n",
    "\n",
    "class PIDController:\n",
    "    \"\"\"\n",
    "    Standard PID Controller implementation.\n",
    "    Calculates the required heat input based on the error.\n",
    "    \"\"\"\n",
    "    def __init__(self, Kp, Ki, Kd, output_limits=(0, 1000)):\n",
    "        self.Kp = Kp\n",
    "        self.Ki = Ki\n",
    "        self.Kd = Kd\n",
    "        self.output_limits = output_limits\n",
    "        self.P_term = 0\n",
    "        self.I_term = 0\n",
    "        self.D_term = 0\n",
    "        self.last_error = 0\n",
    "        self.integral_limit = 200 # Anti-windup limit\n",
    "\n",
    "    def calculate(self, current_temp, setpoint, dt):\n",
    "        error = setpoint - current_temp\n",
    "\n",
    "        # Proportional Term\n",
    "        self.P_term = self.Kp * error\n",
    "\n",
    "        # Integral Term (with anti-windup)\n",
    "        self.I_term += error * dt\n",
    "        self.I_term = max(min(self.I_term, self.integral_limit), -self.integral_limit)\n",
    "        I_output = self.Ki * self.I_term\n",
    "\n",
    "        # Derivative Term\n",
    "        if dt > 0:\n",
    "            derivative = (error - self.last_error) / dt\n",
    "            self.D_term = self.Kd * derivative\n",
    "        else:\n",
    "            self.D_term = 0\n",
    "\n",
    "        # Controller Output (Heat Input, Qin)\n",
    "        output = self.P_term + I_output + self.D_term\n",
    "\n",
    "        # Clamp output to physical limits (e.g., 0 to 1000W)\n",
    "        output = max(min(output, self.output_limits[1]), self.output_limits[0])\n",
    "\n",
    "        # Update last error\n",
    "        self.last_error = error\n",
    "\n",
    "        return output\n",
    "\n",
    "# PHYSICS-BASED SIMULATION MODEL (Lumped Capacitance) ---\n",
    "\n",
    "def simulate_physics_step(current_temp, Q_in, Tamb, dt, tau=500, heat_to_temp_factor=1e-3):\n",
    "    \"\"\"\n",
    "    Simulates one step of the oven's heating using a simplified\n",
    "    Lumped Capacitance/Newton's Law of Cooling model.\n",
    "    This is now used as a BASELINE for comparison.\n",
    "\n",
    "    dT/dt = (1/tau) * (Tamb - T) + Q_in * factor\n",
    "    \"\"\"\n",
    "    # Heat loss term (Newton's Law of Cooling approximation)\n",
    "    loss_rate = (Tamb - current_temp) / tau\n",
    "\n",
    "    # Heater input term (Q_in is power, factor converts to temp rise rate)\n",
    "    heat_rate = Q_in * heat_to_temp_factor\n",
    "\n",
    "    dT = (loss_rate + heat_rate) * dt\n",
    "    new_temp = current_temp + dT\n",
    "    return new_temp\n",
    "\n",
    "# DATA GENERATION FOR TRAINING THE PINN ---\n",
    "\n",
    "def generate_training_data(model_pinn, time_total, T_initial, T_setpoint, dt, pid_controller, sim_physics_step):\n",
    "    \"\"\"\n",
    "    Runs a physics-based simulation to generate T(t) data points\n",
    "    to train the PINN on actual dynamics.\n",
    "    \"\"\"\n",
    "    print(\"Generating synthetic data for PINN training...\")\n",
    "    T_current = T_initial\n",
    "    T_data = [T_initial]\n",
    "    t_data = [0.0]\n",
    "    t = 0.0\n",
    "    steps = int(time_total / dt)\n",
    "\n",
    "    # Use a dummy PID/Physics loop to generate realistic T(t) curve\n",
    "    for _ in range(steps):\n",
    "        Q_in = pid_controller.calculate(T_current, T_setpoint, dt)\n",
    "        T_current = sim_physics_step(T_current, Q_in, 25.0, dt)\n",
    "        t += dt\n",
    "        T_data.append(T_current)\n",
    "        t_data.append(t)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    T_data = torch.tensor(T_data, dtype=torch.float32).unsqueeze(1)\n",
    "    t_data = torch.tensor(t_data, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # Normalize data for PINN (assuming max temp is 250C)\n",
    "    T_max = 250.0\n",
    "    T_data_norm = T_data / T_max\n",
    "    t_data_norm = t_data / time_total\n",
    "\n",
    "    # Sample interior points for the PDE loss\n",
    "    # Generate random points in (t, x) domain\n",
    "    N_interior = 1000\n",
    "    t_interior = torch.rand(N_interior, 1, requires_grad=True)\n",
    "    x_interior = torch.rand(N_interior, 1, requires_grad=True)\n",
    "\n",
    "    print(f\"Data generated: {len(t_data)} points.\")\n",
    "    return t_data, T_data, T_data_norm, t_interior, x_interior, T_max, time_total\n",
    "\n",
    "# PINN TRAINING FUNCTION ---\n",
    "\n",
    "def train_pinn(model_pinn, t_data, T_data_norm, t_interior, x_interior, T_initial_norm, T_setpoint_norm, time_total, T_max):\n",
    "    \"\"\"\n",
    "    Trains the PINN using a combined loss: Data Loss + Physics Loss + IC/BC Loss.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model_pinn.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    epochs = 5000 # Number of training epochs\n",
    "    alpha = 0.01 # Thermal diffusivity constant (normalized for simplicity)\n",
    "\n",
    "    # Combine t_data (normalized) with a fixed x=0.5 (middle of the oven)\n",
    "    t_data_norm = t_data / time_total\n",
    "    x_data = torch.ones_like(t_data_norm) * 0.5\n",
    "    tx_data = torch.cat([t_data_norm, x_data], dim=1)\n",
    "\n",
    "    # Initial Condition (IC) setup: T(t=0, x) = T_initial_norm\n",
    "    t_ic = torch.zeros(100, 1) # 100 points at t=0\n",
    "    x_ic = torch.rand(100, 1)  # distributed across space x\n",
    "    tx_ic = torch.cat([t_ic, x_ic], dim=1)\n",
    "    T_ic_target = torch.ones_like(t_ic) * T_initial_norm\n",
    "\n",
    "    # Boundary Condition (BC) setup: Fixed T at boundaries x=0, x=1\n",
    "    T_bc_target = torch.ones(100, 1) * T_setpoint_norm\n",
    "\n",
    "    # Boundary x=0\n",
    "    t_bc0 = torch.rand(50, 1)\n",
    "    x_bc0 = torch.zeros(50, 1)\n",
    "    tx_bc0 = torch.cat([t_bc0, x_bc0], dim=1)\n",
    "\n",
    "    # Boundary x=1\n",
    "    t_bc1 = torch.rand(50, 1)\n",
    "    x_bc1 = torch.ones(50, 1)\n",
    "    tx_bc1 = torch.cat([t_bc1, x_bc1], dim=1)\n",
    "    \n",
    "    # Combine all boundary points (x=0 and x=1) into one tensor (size 100)\n",
    "    tx_bc = torch.cat([tx_bc0, tx_bc1], dim=0) \n",
    "\n",
    "    # Weights for the combined loss (Tuned for high data fidelity)\n",
    "    w_data = 10.0  # Increased weight to prioritize the accurate data\n",
    "    w_pde = 0.5 \n",
    "    w_ic = 1.0\n",
    "    w_bc = 0.5  # Lower weight to reduce conflict from simplified BC\n",
    "\n",
    "    print(\"Starting PINN training...\")\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # A. Data Loss (Supervised Learning) ---\n",
    "        T_pred_data = model_pinn(tx_data)\n",
    "        loss_data = loss_fn(T_pred_data, T_data_norm)\n",
    "\n",
    "        # B. Physics Loss (PDE Residual) ---\n",
    "        loss_pde = pde_loss(model_pinn, t_interior, x_interior, alpha)\n",
    "\n",
    "        # C. Initial Condition (IC) Loss ---\n",
    "        T_pred_ic = model_pinn(tx_ic)\n",
    "        loss_ic = loss_fn(T_pred_ic, T_ic_target)\n",
    "\n",
    "        # D. Boundary Condition (BC) Loss ---\n",
    "        T_pred_bc = model_pinn(tx_bc)\n",
    "        loss_bc = loss_fn(T_pred_bc, T_bc_target)\n",
    "\n",
    "        # E. Total Loss ---\n",
    "        total_loss = (w_data * loss_data) + (w_pde * loss_pde) + (w_ic * loss_ic) + (w_bc * loss_bc)\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 500 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Total Loss: {total_loss.item():.6f} | \"\n",
    "                  f\"Data: {loss_data.item():.6f} | PDE: {loss_pde.item():.6f} | \"\n",
    "                  f\"IC/BC: {(loss_ic + loss_bc).item():.6f}\")\n",
    "\n",
    "    print(\"PINN training complete.\")\n",
    "    return model_pinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9638d3e0-d6e2-40b4-b77b-1b34457abb44",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# MAIN SIMULATION AND PINN CONTROL ---\n",
    "\n",
    "def run_pinn_controlled_twin():\n",
    "    # --- Parameters ---\n",
    "    T_INITIAL = 25.0      # Initial temperature (°C)\n",
    "    T_SETPOINT = 180.0    # Target temperature (°C)\n",
    "    T_AMBIENT = 25.0      # Ambient temperature (°C)\n",
    "    SIM_TIME_S = 3600.0   # Total simulation time (1 hour in seconds)\n",
    "    DT = 1.0              # Time step (1 second)\n",
    "\n",
    "    # PID constants (tuned for the simple physics model)\n",
    "    KP, KI, KD = 50.0, 0.01, 5.0 \n",
    "\n",
    "    # Initialization ---\n",
    "    # PID instance for data generation and the control loop itself\n",
    "    pid_data_gen = PIDController(Kp=KP, Ki=KI, Kd=KD, output_limits=(0, 1000))\n",
    "    pid_control = PIDController(Kp=KP, Ki=KI, Kd=KD, output_limits=(0, 1000))  # Initialize any accumulated terms to 0\n",
    "    pinn_model = OvenPINN()\n",
    "\n",
    "    # Data Generation & PINN Training ---\n",
    "    # The PINN still needs to be trained on data from the original PID/Physics loop\n",
    "    t_data, T_data, T_data_norm, t_interior, x_interior, T_max, time_total = \\\n",
    "        generate_training_data(pinn_model, SIM_TIME_S, T_INITIAL, T_SETPOINT, DT, pid_data_gen, simulate_physics_step)\n",
    "\n",
    "    T_INITIAL_NORM = T_INITIAL / T_max\n",
    "    T_SETPOINT_NORM = T_SETPOINT / T_max\n",
    "\n",
    "    # Train the PINN\n",
    "    pinn_model = train_pinn(pinn_model, t_data, T_data_norm, t_interior, x_interior, T_INITIAL_NORM, T_SETPOINT_NORM, time_total, T_max)\n",
    "    pinn_model.eval() # Switch to evaluation mode\n",
    "\n",
    "    # PINN-Controlled Digital Twin Simulation ---\n",
    "    \n",
    "    # T_controlled is the current state of the oven, driven by the PINN model\n",
    "    T_controlled = T_INITIAL \n",
    "    # T_phys_baseline runs in parallel for comparison, using the same Q_in\n",
    "    T_phys_baseline = T_INITIAL \n",
    "    \n",
    "    time_series = [0.0]\n",
    "    T_controlled_series = [T_INITIAL]\n",
    "    T_phys_baseline_series = [T_INITIAL]\n",
    "    T_setpoint_series = [T_SETPOINT]\n",
    "    Q_in_series = [0.0]\n",
    "    t = 0.0\n",
    "    steps = int(SIM_TIME_S / DT)\n",
    "\n",
    "    print(\"\\nStarting PINN-Controlled Digital Twin Simulation (PINN replaces Physics Model)...\")\n",
    "\n",
    "    for _ in range(steps):\n",
    "        # PID Controller determines heat input based on the CONTROLLED temperature (T_controlled)\n",
    "        # This is the key change: PID now trusts the PINN's state\n",
    "        Q_in = pid_control.calculate(T_controlled, T_SETPOINT, DT)\n",
    "        Q_in_series.append(Q_in)\n",
    "\n",
    "        # PINN Model is the new state machine (Digital Twin Core)\n",
    "        t_norm = (t + DT) / SIM_TIME_S\n",
    "        x_norm = 0.5 # Predict at the middle of the oven\n",
    "        t_tensor = torch.tensor([[t_norm]], dtype=torch.float32)\n",
    "        x_tensor = torch.tensor([[x_norm]], dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            T_pinn_norm = pinn_model(torch.cat([t_tensor, x_tensor], dim=1)).item()\n",
    "            # Update the controlled system temperature for the next step\n",
    "            T_controlled = T_pinn_norm * T_max \n",
    "\n",
    "        # Physics Model runs in parallel as a BASELINE check, using the same Q_in\n",
    "        T_phys_baseline = simulate_physics_step(T_phys_baseline, Q_in, T_AMBIENT, DT)\n",
    "\n",
    "        # Record Data\n",
    "        t += DT\n",
    "        time_series.append(t / 60.0) # Convert to minutes\n",
    "        T_controlled_series.append(T_controlled)\n",
    "        T_phys_baseline_series.append(T_phys_baseline)\n",
    "        T_setpoint_series.append(T_SETPOINT)\n",
    "\n",
    "    # Visualization ---\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(time_series, T_setpoint_series, 'r--', label='Set Point (Target)')\n",
    "    plt.plot(time_series, T_phys_baseline_series, 'b-', label='Physics Model Baseline (Reference)')\n",
    "    plt.plot(time_series, T_controlled_series, 'g:', linewidth=3, label='PINN-Controlled System (Live)')\n",
    "    plt.title('Digital Twin Oven: PINN as Primary System Model for PID Control')\n",
    "    plt.xlabel('Time (Minutes)')\n",
    "    plt.ylabel('Temperature (°C)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60015ea6-88e9-47ea-8b40-46f808fedcdc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data for PINN training...\n",
      "Data generated: 3601 points.\n",
      "Starting PINN training...\n",
      "Epoch 1/5000 | Total Loss: 3.042528 | Data: 0.287632 | PDE: 0.000471 | IC/BC: 0.328118\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mrun_pinn_controlled_twin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m, in \u001b[0;36mrun_pinn_controlled_twin\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m T_SETPOINT_NORM \u001b[38;5;241m=\u001b[39m T_SETPOINT \u001b[38;5;241m/\u001b[39m T_max\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Train the PINN\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m pinn_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_pinn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpinn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_data_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_interior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_interior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_INITIAL_NORM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_SETPOINT_NORM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_total\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m pinn_model\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m# Switch to evaluation mode\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# PINN-Controlled Digital Twin Simulation ---\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# T_controlled is the current state of the oven, driven by the PINN model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 174\u001b[0m, in \u001b[0;36mtrain_pinn\u001b[1;34m(model_pinn, t_data, T_data_norm, t_interior, x_interior, T_initial_norm, T_setpoint_norm, time_total, T_max)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# E. Total Loss ---\u001b[39;00m\n\u001b[0;32m    172\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m (w_data \u001b[38;5;241m*\u001b[39m loss_data) \u001b[38;5;241m+\u001b[39m (w_pde \u001b[38;5;241m*\u001b[39m loss_pde) \u001b[38;5;241m+\u001b[39m (w_ic \u001b[38;5;241m*\u001b[39m loss_ic) \u001b[38;5;241m+\u001b[39m (w_bc \u001b[38;5;241m*\u001b[39m loss_bc)\n\u001b[1;32m--> 174\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\siona\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\siona\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\siona\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_pinn_controlled_twin()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
