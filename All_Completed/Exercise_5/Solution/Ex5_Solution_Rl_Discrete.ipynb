{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d9063b",
   "metadata": {},
   "source": [
    "#### RL Mine Explorer Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a6baed2-f6b6-45ff-92c2-ff7805f89425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f7a387-e3ab-425d-ae4e-92f3a7d39339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize number of states and actions\n",
    "state_size = 6\n",
    "action_size = 2\n",
    "\n",
    "# Rewards and actions initialization\n",
    "actions = [0,1] # 0 for left and 1 for right\n",
    "\n",
    "global reward\n",
    "reward = [100,0,0,0,0,40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace8267b-a79f-4500-81da-028c53a9ddfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c0b234-2003-409f-abbc-b01d56b709c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the percent you want to explore\n",
    "\n",
    "def action(act,state,epsilon=0.5):\n",
    "    \n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        \"\"\"\n",
    "        Explore: select a random action    \n",
    "        \"\"\"\n",
    "        a1 = random.randint(0,1)\n",
    "        return a1\n",
    "\n",
    "    else:\n",
    "        \"\"\"\n",
    "        Exploit: select the action with max value (future reward)    \n",
    "        \"\"\"\n",
    "        a2 = np.argmax(Q[state])\n",
    "        return int(a2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b7b07c8-06d3-47bf-8284-f76500a1d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning part\n",
    "# Update q values as per the Bellman equation for computing Q values\n",
    "\n",
    "def q_update(state,action,episode,lr,gamma):\n",
    "    if 100 != np.sum(Q[state]):      # Check if we are in terminal state\n",
    "        if 40 != np.sum(Q[state]):   # Check if we are in terminal state\n",
    "            if action == 0:\n",
    "                Q[state, action] = Q[state, action] + lr * (reward[state] + gamma * np.max(Q[state-1, :]) - Q[state, action])\n",
    "                return Q[state,action],state-1\n",
    "                \n",
    "            else:\n",
    "                Q[state, action] = Q[state, action] + lr * (reward[state] + gamma * np.max(Q[state+1, :]) - Q[state, action])\n",
    "                return Q[state,action],state+1\n",
    "        else:\n",
    "            #print(f\"End of episode at T2:{episode}\")\n",
    "            return 'T2'\n",
    "    else:\n",
    "        #print(f\"End of episode at T1:{episode}\")\n",
    "        return 'T1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf222014-24d4-4cd9-aaa2-45e33efa90a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The terms in the above equation and their significance ####\n",
    "\n",
    "- Learning Rate: lr or learning rate, often referred to as alpha or α, can simply be defined as how much you accept the new value vs the old value. Above we are taking the difference between new and old and then multiplying that value by the learning rate. This value then gets added to our previous q-value which essentially moves it in the direction of our latest update.\n",
    "\n",
    "- Gamma: gamma or γ is a discount factor. It’s used to balance immediate and future reward. From our update rule above you can see that we apply the discount to the future reward. Typically this value can range anywhere from 0.8 to 0.99.\n",
    "\n",
    "- Reward: reward is the value received after completing a certain action at a given state. A reward can happen at any given time step or only at the terminal time step.\n",
    "\n",
    "- Max: np.max() uses the numpy library and is taking the maximum of the future reward and applying it to the reward for the current state. What this does is impact the current action by the possible future reward. This is the beauty of q-learning. We’re allocating future reward to current actions to help the agent select the highest return action at any given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0997541f-a4c5-46b8-a2f9-03a5e875bf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting learning.....\n",
      "\n",
      "End of 1000 Episodes\n",
      "\n",
      "Final Q-values:\n",
      "[[100.     0.  ]\n",
      " [ 50.    12.5 ]\n",
      " [ 25.     6.25]\n",
      " [ 12.5   10.  ]\n",
      " [  6.25  20.  ]\n",
      " [  0.    40.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Q table\n",
    "Q = np.zeros((state_size, action_size))\n",
    "Q[5,1] = 40\n",
    "Q[0,0] = 100\n",
    "lr = 0.1\n",
    "gamma = 0.5\n",
    "\n",
    "# Run episodes\n",
    "print(\"Starting learning.....\")\n",
    "for ep in range(1000):   \n",
    "    ss = int(3) # Start state\n",
    "    f_action = random.randint(0,1) # Take any first action\n",
    "    \n",
    "    q  = q_update(int(ss),int(f_action),ep,lr=lr,gamma=gamma)  # Q value and new state is returned by the update function\n",
    "\n",
    "    if q == 'T1' or q == 'T2':\n",
    "        break\n",
    "    \n",
    "    t_state=0\n",
    "    while t_state <= 0:  # Loops till end of episode\n",
    "        s = q            # Store the value of the returned from q-update after first action in a new variable\n",
    "        \n",
    "        n_action = action(act=actions,state=int(s[1]),epsilon=0.8) # Pick next action        \n",
    "        q  = q_update(int(s[1]),int(n_action),ep,lr=lr,gamma=gamma) # Q value and new state is returned\n",
    "        if q == 'T1' or q == 'T2':    # terminal states\n",
    "            t_state = 1\n",
    "            break\n",
    "        \n",
    "print(f\"\\nEnd of {ep+1} Episodes\\n\\nFinal Q-values:\\n{Q}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b3ae427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path Traversed:\n",
      "3 ↓\n",
      "2 ↓\n",
      "1 ↓\n",
      "0 ↓\n",
      "T1 ↓\n"
     ]
    }
   ],
   "source": [
    "# Let us deploy and test the optimal policy\n",
    "\n",
    "optimal_policy = Q # Trained policy\n",
    "\n",
    "actions = [0,1] # 0 for left and 1 for right\n",
    "states = [] # Store the path traversed\n",
    "\n",
    "def navigate(state):\n",
    "    if 100 != np.sum(optimal_policy[int(state)]):      # Check if we are in terminal state\n",
    "        if 40 != np.sum(optimal_policy[state]):   # Check if we are in terminal state]\n",
    "            \n",
    "            if np.argmax(optimal_policy[state]) == 0:   # Check if max Q value is towards left \n",
    "                state = state - 1\n",
    "                return state    \n",
    "            else:                          # Go right\n",
    "                state = state + 1\n",
    "                return state\n",
    "        else:\n",
    "            return 'T2'\n",
    "    else:\n",
    "        return 'T1'\n",
    "\n",
    "# If we feed it a random start state it should be able to navigate for max returns\n",
    "start_state = random.randint(1,4)\n",
    "states.append(start_state) # Load the path with start state\n",
    "    \n",
    "# Call navigation function    \n",
    "next_state = navigate(start_state)\n",
    "states.append(next_state)\n",
    "\n",
    "# Navigate till a terminal state is reached\n",
    "while next_state != 'T1' or next_state != 'T2':\n",
    "    state = next_state               # Set next state to current state\n",
    "    if next_state == 'T1' or next_state == 'T2':\n",
    "        break\n",
    "    next_state = navigate(state)     # Take the next step\n",
    "    states.append(next_state)\n",
    "\n",
    "# Render path \n",
    "print('Path Traversed:') \n",
    "for s in states:\n",
    "    print(s,'\\u2193')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ddcd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
