{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a4ef9d-8e2f-4cb9-bd69-dab2982fdc5e",
   "metadata": {},
   "source": [
    "#### RL in continuous spaces with discrete actions - using DQN ####\n",
    "\n",
    "Cartpole problem - A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right \n",
    "direction on the cart.\n",
    "\n",
    "Check the deatils for this environment here - https://gymnasium.farama.org/environments/classic_control/cart_pole/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79436a89-7ca8-48c4-afe7-6c0b21172f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam,SGD\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab823890-d291-4f56-8073-1b1083e44849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5fe0b6-6d75-4f22-81dc-32d5caf52261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "150d375c-0c1e-4aa1-a49d-5fbb13cf23db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x17ba8643c30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78bae30a-4bed-4c7d-bbc2-5df171512bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9df551b-ccf0-45e8-8c25-85ff3caefe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad33c5-4c74-4181-99af-cae58d704b5c",
   "metadata": {},
   "source": [
    "### Description\n",
    "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
    "    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
    "    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
    "     in the left and right direction on the cart.\n",
    "    ### Action Space\n",
    "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
    "     of the fixed force the cart is pushed with.\n",
    "    | Num | Action                 |\n",
    "    |-----|------------------------|\n",
    "    | 0   | Push cart to the left  |\n",
    "    | 1   | Push cart to the right |\n",
    "    **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
    "     the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "    ### Observation Space\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "    | Num | Observation           | Min                 | Max               |\n",
    "    |-----|-----------------------|---------------------|-------------------|\n",
    "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
    "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can take values between `(-4.8, 4.8)`, but the episode terminates\n",
    "       if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
    "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "    ### Rewards\n",
    "    Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\n",
    "    including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
    "    ### Starting State\n",
    "    All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
    "    ### Episode End\n",
    "    The episode ends if any one of the following occurs:\n",
    "    1. Termination: Pole Angle is greater than ±12°\n",
    "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "    ### Arguments\n",
    "    ```\n",
    "    gym.make('CartPole-v1')\n",
    "    ```\n",
    "    No additional arguments are currently supported.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3cf571c-a08e-4f7f-abc4-8bd746f76376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6387d7a3-5c1d-40d3-bf78-92b2b910d5ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base model\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.lin1 = nn.Linear(4,16)     # 4 input features as there are 4 states\n",
    "        self.lin2 = nn.Linear(16,16)\n",
    "        self.lin3 = nn.Linear(16,2)     # 2 outputs as there are 2 actions\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "    \n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40131941-d67d-427b-8046-0e1be8ffad8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (lin1): Linear(in_features=4, out_features=16, bias=True)\n",
       "  (lin2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (lin3): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the DQNET and create targetNet\n",
    "# Make sure its a deepcopy else changes to one will affect the other\n",
    "\n",
    "DQNet = Model().to(device)\n",
    "targetNet = copy.deepcopy(DQNet).to(device)\n",
    "\n",
    "# targetNet is always in eval mode - no back propogation\n",
    "targetNet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b169a870-c4db-4503-9f07-e354ba2c2521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 16]              80\n",
      "            Linear-2                [-1, 1, 16]             272\n",
      "            Linear-3                 [-1, 1, 2]              34\n",
      "================================================================\n",
      "Total params: 386\n",
      "Trainable params: 386\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# You could look at the model summary\n",
    "\n",
    "summary(targetNet,(1,4))\n",
    "# summary(DQNet,(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04722cad-4f9f-4b0c-9188-c0aee99b8919",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.], device='cuda:0')\n",
      "tensor([0.0982, 0.2028], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "tensor([0.0982, 0.2028], device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the models and see if they give the same output\n",
    "\n",
    "input = torch.from_numpy(np.array([1,2,3,4])).float().to(device)\n",
    "print(input)\n",
    "print(DQNet(input))\n",
    "print(targetNet(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "618d406c-b487-4906-93ce-63ee9b8fc467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize parameters for network\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = Adam(params=DQNet.parameters(), lr=lr)   # We only train DQN\n",
    "\n",
    "# E-Greedy policy but as we explore deeper we take les random actions\n",
    "eps_start = 1\n",
    "eps_end = 0.0001\n",
    "eps_decay = 0.999\n",
    "\n",
    "loss_fn = nn.MSELoss()  # Simple MEan Square Error function\n",
    "episodes = 500\n",
    "mini_batch_size = 128\n",
    "\n",
    "loss_value = 0\n",
    "loss_history = []\n",
    "update_freq = 15       # Updating target network\n",
    "gamma = 0.95\n",
    "scores = []\n",
    "rewards = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83d7967b-3e00-401f-a66f-d4b4bf9d53a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def training(Qnet,t_net,replay_memory,optimizer,loss_fn,mini_batch_size=32):\n",
    "    \n",
    "    # Sample random observations\n",
    "    observations = random.choices(replay_memory,k=mini_batch_size)\n",
    "        \n",
    "    Qnet.train()\n",
    "    \n",
    "    for epochs in range(1): \n",
    "        \n",
    "        # Loop over sampled observations\n",
    "        for observation in observations:\n",
    "            \n",
    "            state = torch.from_numpy(np.array(observation[0])).float().to(device)  # Convert to tensor and put on device\n",
    "\n",
    "            # Predict Q-value at time t and next action\n",
    "            q_values = Qnet(state)\n",
    "            #expected_value = q_values.detach().numpy()[int(observation[2])]\n",
    "            expected_value = q_values[int(observation[1])]\n",
    "            \n",
    "            #expected_value = torch.tensor(expected_value,requires_grad=True)\n",
    "\n",
    "            done = observation[4]\n",
    "\n",
    "            # Determine Q-value at time t+1\n",
    "            next_state = torch.from_numpy(np.array(observation[3])).float() \n",
    "            next_q_values = t_net(next_state.to(device))          \n",
    "            next_action = torch.argmax(next_q_values)\n",
    "            next_q_value = next_q_values[next_action]       \n",
    "\n",
    "            # Bellman's eqution for current state\n",
    "            if done:\n",
    "                target_value = torch.tensor(observation[2])   # If episode is done the target value is the reward\n",
    "                \n",
    "            else:\n",
    "                target_value = observation[2] + (gamma * next_q_value)   # Add direct reward to obtain target value\n",
    "                \n",
    "            # Compute loss value\n",
    "            loss = loss_fn(expected_value.to(device), target_value.to(device))\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            # Back prop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "    return Qnet,loss_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb5fe595-a20a-4dd3-9815-19501ade4550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/500 Rewards:22.0 Buffer Length:22 and eps:1\n",
      "Episode 10/500 Rewards:45.0 Buffer Length:298 and eps:1\n",
      "Episode 20/500 Rewards:30.0 Buffer Length:549 and eps:1\n",
      "Episode 30/500 Rewards:35.0 Buffer Length:850 and eps:1\n",
      "Episode 40/500 Rewards:13.0 Buffer Length:1046 and eps:1\n",
      "Episode 50/500 Rewards:19.0 Buffer Length:1279 and eps:1\n",
      "Episode 60/500 Rewards:14.0 Buffer Length:1460 and eps:1\n",
      "Episode 70/500 Rewards:25.0 Buffer Length:1686 and eps:1\n",
      "Episode 80/500 Rewards:20.0 Buffer Length:1833 and eps:1\n",
      "Episode 90/500 Rewards:14.0 Buffer Length:1985 and eps:1\n",
      "Episode 100/500 Rewards:10.0 Buffer Length:2267 and eps:0.7655707927460921\n",
      "Episode 110/500 Rewards:70.0 Buffer Length:2624 and eps:0.5356297035976458\n",
      "Episode 120/500 Rewards:279.0 Buffer Length:3979 and eps:0.1380705958965304\n",
      "Episode 130/500 Rewards:501.0 Buffer Length:7049 and eps:0.006418796221744962\n",
      "Episode 140/500 Rewards:112.0 Buffer Length:9734 and eps:0.00043906368915942414\n",
      "Episode 150/500 Rewards:100.0 Buffer Length:10679 and eps:0.00017057441704837527\n",
      "Episode 160/500 Rewards:107.0 Buffer Length:11760 and eps:0.0001\n",
      "Episode 170/500 Rewards:100.0 Buffer Length:12664 and eps:0.0001\n",
      "Episode 180/500 Rewards:224.0 Buffer Length:13507 and eps:0.0001\n",
      "Episode 190/500 Rewards:393.0 Buffer Length:16412 and eps:0.0001\n",
      "Episode 200/500 Rewards:258.0 Buffer Length:19807 and eps:0.0001\n",
      "201\n",
      "202\n",
      "203\n",
      "208\n",
      "209\n",
      "Episode 210/500 Rewards:165.0 Buffer Length:23942 and eps:0.0001\n",
      "Episode 220/500 Rewards:106.0 Buffer Length:25133 and eps:0.0001\n",
      "225\n",
      "Episode 230/500 Rewards:153.0 Buffer Length:26844 and eps:0.0001\n",
      "240\n",
      "Episode 240/500 Rewards:501.0 Buffer Length:28739 and eps:0.0001\n",
      "Episode 250/500 Rewards:101.0 Buffer Length:29713 and eps:0.0001\n",
      "255\n",
      "Episode 260/500 Rewards:203.0 Buffer Length:31573 and eps:0.0001\n",
      "270\n",
      "Episode 270/500 Rewards:501.0 Buffer Length:33604 and eps:0.0001\n",
      "271\n",
      "272\n",
      "275\n",
      "279\n",
      "280\n",
      "Episode 280/500 Rewards:501.0 Buffer Length:37136 and eps:0.0001\n",
      "283\n",
      "284\n",
      "289\n",
      "Episode 290/500 Rewards:108.0 Buffer Length:39756 and eps:0.0001\n",
      "292\n",
      "296\n",
      "Episode 300/500 Rewards:111.0 Buffer Length:42640 and eps:0.0001\n",
      "Episode 310/500 Rewards:108.0 Buffer Length:43820 and eps:0.0001\n",
      "Episode 320/500 Rewards:116.0 Buffer Length:45293 and eps:0.0001\n",
      "321\n",
      "323\n",
      "324\n",
      "327\n",
      "328\n",
      "Episode 330/500 Rewards:52.0 Buffer Length:48795 and eps:0.0001\n",
      "337\n",
      "338\n",
      "Episode 340/500 Rewards:173.0 Buffer Length:50346 and eps:0.0001\n",
      "Episode 350/500 Rewards:112.0 Buffer Length:51380 and eps:0.0001\n",
      "Episode 360/500 Rewards:27.0 Buffer Length:52242 and eps:0.0001\n",
      "Episode 370/500 Rewards:134.0 Buffer Length:53734 and eps:0.0001\n",
      "Episode 380/500 Rewards:31.0 Buffer Length:54647 and eps:0.0001\n",
      "Episode 390/500 Rewards:21.0 Buffer Length:54876 and eps:0.0001\n",
      "Episode 400/500 Rewards:43.0 Buffer Length:55508 and eps:0.0001\n",
      "Episode 410/500 Rewards:104.0 Buffer Length:56300 and eps:0.0001\n",
      "Episode 420/500 Rewards:133.0 Buffer Length:57260 and eps:0.0001\n",
      "Episode 430/500 Rewards:107.0 Buffer Length:58243 and eps:0.0001\n",
      "Episode 440/500 Rewards:99.0 Buffer Length:59649 and eps:0.0001\n",
      "Episode 450/500 Rewards:229.0 Buffer Length:61600 and eps:0.0001\n",
      "Episode 460/500 Rewards:125.0 Buffer Length:62649 and eps:0.0001\n",
      "Episode 470/500 Rewards:73.0 Buffer Length:63464 and eps:0.0001\n",
      "Episode 480/500 Rewards:69.0 Buffer Length:64173 and eps:0.0001\n",
      "Episode 490/500 Rewards:86.0 Buffer Length:65059 and eps:0.0001\n"
     ]
    }
   ],
   "source": [
    "# Q explore / exploit loop\n",
    "\n",
    "eps = eps_start\n",
    "replay = []\n",
    "\n",
    "for i in range(episodes):   \n",
    "    s = env.reset()                         # Reset to start state\n",
    "    s = torch.from_numpy(s[0])              # Convert State to tensor a\n",
    "    done=False                              # Episode end flag\n",
    "    rewards = 0                             # Container for rewards accumulation\n",
    "    t_step = 0    \n",
    "    \n",
    "    while not done: \n",
    "            DQNet.eval()\n",
    "\n",
    "            s = s.to(device)                # Put to device\n",
    "        \n",
    "            # Predict Q-value at time t\n",
    "            q_values = DQNet(s)\n",
    "            \n",
    "            # Take action based on DQNet prediction or random\n",
    "            if np.random.random() < eps:  \n",
    "                a = env.action_space.sample()\n",
    "                \n",
    "                new_state,reward,done,_,_ = env.step(a)\n",
    "                \n",
    "            else:\n",
    "                a = torch.argmax(q_values)\n",
    "                new_state,reward,done,_,_ = env.step(a.item())\n",
    "                \n",
    "            # Gather new experience and append to replay buffer   \n",
    "            new_experience = s.tolist(), a, reward, new_state.tolist(), done \n",
    "            replay.append(new_experience)\n",
    "            \n",
    "            # Limit repaly buffer to 100000\n",
    "            if len(replay) >100000:\n",
    "                replay.pop(0)\n",
    "                \n",
    "            # Accumulate rewards\n",
    "            rewards += reward\n",
    "             \n",
    "            s = torch.from_numpy(new_state) # Next state becomes current state\n",
    "\n",
    "            # # decrease the epsilon\n",
    "            # eps = max(eps*eps_decay,eps_end)                 \n",
    "            \n",
    "            # Swap weights from DQNet\n",
    "            if i % update_freq == 0:\n",
    "                targetNet = copy.deepcopy(DQNet)\n",
    "                targetNet.eval()\n",
    "                \n",
    "            # Don't let the episode run more than 500 time steps\n",
    "            t_step += 1\n",
    "            if t_step > 500:\n",
    "                #print('here')\n",
    "                break\n",
    "            \n",
    "            # Train the DQNet that approximates q(s,a), using the replay memory\n",
    "            if len(replay) > 2000:\n",
    "                DQNet,loss = training(Qnet=DQNet,t_net=targetNet, replay_memory=replay, optimizer=optimizer, loss_fn=loss_fn, mini_batch_size=mini_batch_size)\n",
    "                loss_history.append(loss)   # Save the loss value\n",
    "                \n",
    "                # decrease the epsilon\n",
    "                eps = max(eps*eps_decay,eps_end)\n",
    "            \n",
    "    scores.append(rewards)\n",
    "    if rewards >= 500 and i > 200:\n",
    "        torch.save(DQNet,'./saved_models/DQN'+str(i)+'pth')\n",
    "        torch.save(DQNet.state_dict(), 'DQN_parameters.pth')\n",
    "        print(i)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Episode {i}/{episodes} Rewards:{rewards} Buffer Length:{len(replay)} and eps:{eps}\")\n",
    "    #print(rewards,end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c129b9-4710-488d-b8e8-b9cf336ea608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "env = gym.make('CartPole-v1',render_mode='human')\n",
    "model_500 = torch.load('DQN1.pth').to(device)\n",
    "#model_500.load_state_dict(torch.load('DQN_parameters.pth'))\n",
    "model_500.eval()\n",
    "\n",
    "for e in range(10):\n",
    "    sta = env.reset()[0]\n",
    "    \n",
    "    sta = torch.from_numpy(sta)\n",
    "    done = False\n",
    "    i = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = torch.argmax(model_500(sta.to(device)))\n",
    "        new_sta, rew, done,info, _ = env.step(action.item())\n",
    "        sta = torch.from_numpy(new_sta)\n",
    "        if done:\n",
    "            print(e, i)\n",
    "            #break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33fc872-3ec0-43c0-bdc4-f1183774b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf7b21-31cf-4348-9297-edb7d0a3c01e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usg_gym",
   "language": "python",
   "name": "usg_gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2fe4169dbbd5ec058fe1ce8595230ad8089b4ecf39a6ab54513fe4ff962a8576"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
