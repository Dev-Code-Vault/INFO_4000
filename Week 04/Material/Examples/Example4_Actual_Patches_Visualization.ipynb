{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c5f746",
   "metadata": {},
   "source": [
    "### Visualize the patches that a ViT creates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75730c9c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import ViTImageProcessor,ViTModel\n",
    "\n",
    "import logging\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9c42c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def visualize_vit_patches(image_path: str, model_name: str = \"google/vit-base-patch16-224\"):\n",
    "    \"\"\"\n",
    "    Loads a pretrained ViTImageProcessor, preprocesses an image, and visualizes the\n",
    "    raw 2D patches before they are converted to vectors. This version uses a robust\n",
    "    method to ensure the visualization grid is always correctly sized.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The file path to the image to visualize.\n",
    "        model_name (str): The name of the pretrained Vision Transformer model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the image processor.\n",
    "        # This processor will automatically handle resizing the image to 224x224\n",
    "        # and normalizing its pixel values.\n",
    "        image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "        # Load the image and ensure it's in RGB format.\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Preprocess the image using the image processor.\n",
    "        inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"]\n",
    "        \n",
    "        # Get the patch size from the model's configuration.\n",
    "        patch_size = image_processor.patch_size if hasattr(image_processor, 'patch_size') else 16\n",
    "        \n",
    "        # Manually extract the patches from the preprocessed tensor.\n",
    "        # This is where we \"unfold\" the image into non-overlapping patches.\n",
    "        patches_tensor = pixel_values.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "\n",
    "        # Reshape the tensor to a list of individual patches.\n",
    "        patches_tensor = patches_tensor.permute(0, 2, 3, 1, 4, 5).reshape(-1, 3, patch_size, patch_size)\n",
    "\n",
    "        # Use a direct and robust calculation for the grid dimensions.\n",
    "        # Since the image processor has correctly resized the image to 224x224,\n",
    "        # we can explicitly calculate the number of patches per side.\n",
    "        num_patches_per_side = 224 // patch_size\n",
    "        num_patches_h = num_patches_per_side\n",
    "        num_patches_w = num_patches_per_side\n",
    "        total_patches = num_patches_h * num_patches_w\n",
    "\n",
    "        # Get the correct visualization range.\n",
    "        # The pixel values have been normalized and may contain values outside of\n",
    "        # the standard  range. We find the true min and max to prevent clipping.\n",
    "        min_val = patches_tensor.min().item()\n",
    "        max_val = patches_tensor.max().item()\n",
    "        \n",
    "\n",
    "        # Visualize all 196 patches in a grid.\n",
    "        # This will now always correctly create a 14x14 grid.\n",
    "        fig, axes = plt.subplots(num_patches_h, num_patches_w, figsize=(14, 14))\n",
    "        fig.suptitle(f\"ViT Patches from {model_name} ({total_patches} Patches)\", fontsize=16)\n",
    "\n",
    "        for i, ax in enumerate(axes.flatten()):\n",
    "            if i < total_patches:\n",
    "                # Permute the tensor from (C, H, W) to (H, W, C) for Matplotlib.\n",
    "                patch_to_display = patches_tensor[i].permute(1, 2, 0)\n",
    "                \n",
    "                # Use vmin and vmax to correctly display the normalized data range\n",
    "                # without clipping to black or white.\n",
    "                \n",
    "                logger = logging.getLogger() # These lines are just to block the clipping messaages when plotting\n",
    "                old_level = logger.level\n",
    "                logger.setLevel(100)\n",
    "\n",
    "                # plotting code here\n",
    "\n",
    "                ax.imshow(patch_to_display.numpy(), vmin=min_val, vmax=max_val)\n",
    "                ax.set_title(f\"Patch {i}\")\n",
    "                ax.axis('off')\n",
    "                \n",
    "                logger.setLevel(old_level)\n",
    "    \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Please ensure you have a valid image path and the 'transformers', 'torch', and 'Pillow' libraries are installed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4977305f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Example usage: Replace 'path/to/your/image.jpg' with a local image file path.\n",
    "# This code will correctly handle your 225x225 image and display all 196 patches.\n",
    "# visualize_vit_patches(\"path/to/your/image.jpg\")\n",
    "visualize_vit_patches(\"cat.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7cbf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Example usage: Replace 'path/to/your/image.jpg' with a local image file path.\n",
    "# This code will correctly handle your 225x225 image and display all 196 patches.\n",
    "# visualize_vit_patches(\"path/to/your/image.jpg\")\n",
    "visualize_vit_patches(\"dog.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3916ce0c",
   "metadata": {},
   "source": [
    "### After patching, the patches (tokens) are embedded into a encoded representation - Let us look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d795f03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to do that\n",
    "\n",
    "def get_vit_patch_embeddings(image_path: str, model_name: str = \"google/vit-base-patch16-224\"):\n",
    "    \"\"\"\n",
    "    Loads a pretrained ViT model and image, and extracts the patch embeddings\n",
    "    after the initial patching and linear projection.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The file path to the image to visualize.\n",
    "        model_name (str): The name of the pretrained Vision Transformer model.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The tensor of patch embeddings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Load the image processor and the model\n",
    "        image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "        model = ViTModel.from_pretrained(model_name)\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "\n",
    "        # Step 2: Load and preprocess the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        # Step 3: Pass the preprocessed image tensor to the model.\n",
    "        # We set `output_hidden_states=True` to get intermediate outputs.\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        # Step 4: Extract the embeddings.\n",
    "        # The `last_hidden_state` contains the sequence of all tokens (CLS + patches).\n",
    "        # Its shape will be (batch_size, num_patches + 1, hidden_size), e.g., (1, 197, 768).\n",
    "        # The first token is the CLS token, so we slice it off to get only the patches.\n",
    "        # The `last_hidden_state` is the output of the final encoder layer.\n",
    "        patch_embeddings = outputs.last_hidden_state[:, 1:, :]\n",
    "\n",
    "        print(f\"\\nShape of the extracted patch embeddings: {patch_embeddings.shape}\")\n",
    "        \n",
    "        return patch_embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extracted embeddings of cat\n",
    "cat_embeddings = get_vit_patch_embeddings(\"cat.jpg\")\n",
    "print(f\"Sample of patch embedding:\\n{cat_embeddings[0][0]}\\nand\\nsize of each vector of a given patch: {len(cat_embeddings[0][0])}\")\n",
    "\n",
    "# Extracted embeddins of dog\n",
    "dog_embeddings = get_vit_patch_embeddings(\"dog.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec6f76",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# PCA for reducing dimensions so we can visualize\n",
    "\n",
    "def visualize_embeddings_with_pca(cat_embeddings, dog_embeddings):\n",
    "    \"\"\"\n",
    "    Applies PCA to embeddings from a cat and a dog image and visualizes the results\n",
    "    to show how the model has clustered the patches.\n",
    "\n",
    "    Args:\n",
    "        cat_embeddings (torch.Tensor): A tensor of patch embeddings for the cat image,\n",
    "                                       with shape (1, 196, 768).\n",
    "        dog_embeddings (torch.Tensor): A tensor of patch embeddings for the dog image,\n",
    "                                       with shape (1, 196, 768).\n",
    "    \"\"\"\n",
    "    # Step 1: Prepare the data for PCA.\n",
    "    # The PCA algorithm from scikit-learn expects a 2D array of shape (n_samples, n_features).\n",
    "    # We have two tensors, each with a shape of (1, 196, 768).\n",
    "    # We need to flatten this into a single array where each row is a patch embedding.\n",
    "    # We will combine the patches from both images into one dataset for PCA.\n",
    "    \n",
    "    # Squeeze the batch dimension and convert to NumPy arrays for scikit-learn.\n",
    "    cat_patches = cat_embeddings.squeeze(0).numpy()  # Shape: (196, 768)\n",
    "    dog_patches = dog_embeddings.squeeze(0).numpy()  # Shape: (196, 768)\n",
    "    \n",
    "    # Concatenate the arrays vertically to create a single dataset for PCA.\n",
    "    # The first 196 rows are for the cat, the next 196 are for the dog.\n",
    "    all_patches = np.concatenate((cat_patches, dog_patches), axis=0)\n",
    "    print(f\"Shape of combined data for PCA: {all_patches.shape}\")\n",
    "\n",
    "    # Step 2: Apply PCA for dimensionality reduction.\n",
    "    # We reduce the dimensionality from 768 to 2 so we can plot it on a 2D scatter plot.\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_patches = pca.fit_transform(all_patches)\n",
    "    print(f\"Shape of data after PCA: {reduced_patches.shape}\")\n",
    "\n",
    "    # Step 3: Visualize the results.\n",
    "    # We plot the reduced embeddings, coloring the cat patches differently from the dog patches.\n",
    "    # The first 196 points correspond to the cat, the next 196 to the dog.\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(reduced_patches[:196, 0], reduced_patches[:196, 1], c='blue', alpha=0.5, label='Cat Patches')\n",
    "    plt.scatter(reduced_patches[196:, 0], reduced_patches[196:, 1], c='orange', alpha=0.5, label='Dog Patches')\n",
    "    \n",
    "    plt.title('PCA of ViT Patch Embeddings (Cat vs. Dog)')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage:\n",
    "# Assuming you have already obtained the embeddings for a cat and a dog image\n",
    "# using the method from our previous conversation.\n",
    "#\n",
    "# Replace the lines below with your actual embedding tensors.\n",
    "#\n",
    "# For demonstration purposes, we create dummy tensors with the correct shape.\n",
    "# cat_emb = torch.randn(1, 196, 768)\n",
    "# dog_emb = torch.randn(1, 196, 768)\n",
    "#\n",
    "visualize_embeddings_with_pca(cat_embeddings,dog_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_torch",
   "language": "python",
   "name": "py312_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
