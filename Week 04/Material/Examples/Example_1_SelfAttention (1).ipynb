{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff003d5-bee0-41b5-8ce7-3a620f2fe161",
   "metadata": {},
   "source": [
    "### Transformer self-attention concept \n",
    "\n",
    "In transformers, the attention mechanism allows the model to focus on different parts of the input sequence when producing an output. The scaled dot-product attention is defined as:\n",
    "\n",
    "$Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "Where:\n",
    "\n",
    "- Q: Query matrix\n",
    "- K: Key matrix\n",
    "- V: Value matrix\n",
    "- $d_k$: Dimension of the key vectors\n",
    "\n",
    "Let us implement an example and see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c6352-12c0-4ea6-8185-f4403af7a3c2",
   "metadata": {},
   "source": [
    "### Attention with Embeddings and Positional Encoding\n",
    "\n",
    "This example will demonstrate 3 concepts:\n",
    "\n",
    "1. How attention is computed?\n",
    "2. How computing attention with positional encoding makes a difference?\n",
    "3. How multi-head attention works?\n",
    "\n",
    "Start with Defining Embeddings and Positional Encoding\n",
    "\n",
    "- Let’s use a simple sentence: \"I love dogs\".\n",
    "- We’ll represent each word as a 4-dimensional embedding and add positional encoding.\n",
    "- Word Embeddings (Random for Illustration)\n",
    "  \n",
    "Word,Embedding:\n",
    "-- I,[1, 0, 0, 0]\n",
    "-- love,[0, 1, 0, 0]\n",
    "-- dogs,[0, 0, 1, 0]\n",
    "\n",
    "Positional Encoding (Simplified):\n",
    "\n",
    "We’ll use a simple sinusoidal positional encoding for positions 0, 1, and 2 without going into details, it is the distance between tokens):\n",
    "\n",
    "- Position 0: [0, 0, 0, 0]\n",
    "- Position 1: [0.5, 0.5, 0.5, 0.5]\n",
    "- Position 2: [1, 1, 1, 1]\n",
    "\n",
    "Combined Input:\n",
    "\n",
    "Word Embedding + Positional Encoding\n",
    "- I,[1, 0, 0, 0] + [0, 0, 0, 0] = [1, 0, 0, 0]\n",
    "- love,[0, 1, 0, 0] + [0.5, 0.5, 0.5, 0.5] = [0.5, 1.5, 0.5, 0.5]\n",
    "- dogs,[0, 0, 1, 0] + [1, 1, 1, 1] = [1, 1, 2, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254f0f59-6cf5-43af-b4d1-5d20ac40b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions required to compute the attention scores and creating the Q,K,V vectors\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    # Ensure Q, K, V are 3D arrays with shape (batch_size, seq_len, d_model)\n",
    "    # If not, add a batch dimension\n",
    "    if Q.ndim == 2:\n",
    "        Q = Q[np.newaxis, :, :]  # Add batch dimension\n",
    "        K = K[np.newaxis, :, :]\n",
    "        V = V[np.newaxis, :, :]\n",
    "\n",
    "    # Compute QK^T\n",
    "    matmul_qk = np.matmul(Q, K.transpose(0, 2, 1))  # Shape: (batch_size, seq_len, seq_len)\n",
    "\n",
    "    # Scale by sqrt(d_k)\n",
    "    dk = K.shape[-1]\n",
    "    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
    "\n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = softmax(scaled_attention_logits)  # Shape: (batch_size, seq_len, seq_len)\n",
    "\n",
    "    # Multiply attention weights with V\n",
    "    output = np.matmul(attention_weights, V)  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "# Embeddings\n",
    "embeddings = np.array([\n",
    "    [1, 0, 0, 0],  # I\n",
    "    [0, 1, 0, 0],  # love\n",
    "    [0, 0, 1, 0],  # dogs\n",
    "])\n",
    "\n",
    "# Positional encoding\n",
    "positional_encoding = np.array([\n",
    "    [0, 0, 0, 0],      # Position 0\n",
    "    [0.5, 0.5, 0.5, 0.5],  # Position 1\n",
    "    [1, 1, 1, 1],      # Position 2\n",
    "])\n",
    "\n",
    "# Combined input\n",
    "combined = embeddings + positional_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d2784b-ec45-41a0-a8a4-b8719e53f57f",
   "metadata": {},
   "source": [
    "#### Attention without positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df53478-99e0-4c68-bac9-81678a36dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention without positional encoding\n",
    "Q_no_pos = K_no_pos = V_no_pos = embeddings\n",
    "output_no_pos, attention_weights_no_pos = scaled_dot_product_attention(Q_no_pos, K_no_pos, V_no_pos)\n",
    "\n",
    "print(\"Attention Weights Without Positional Encoding:\\n\", attention_weights_no_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad74fd-6544-4d34-bd80-199c831b08d0",
   "metadata": {},
   "source": [
    "#### How to Read the Matrix\n",
    "\n",
    "1. Rows represent the query word (the word that is \"attending\").\n",
    "2. Columns represent the key word (the word being attended to).\n",
    "3. Each value is a probability (sums to 1 for each row).\n",
    "\n",
    "Let us plot it in a more readable manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430c6e5-40fd-45d1-ace0-052fb16b2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention weights without positional encoding\n",
    "attention_weights = attention_weights_no_pos[0]\n",
    "\n",
    "# Labels for the words\n",
    "words = [\"I\", \"love\", \"dogs\"]\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(\n",
    "    attention_weights,\n",
    "    annot=True,\n",
    "    cmap=\"YlGnBu\",\n",
    "    xticklabels=words,\n",
    "    yticklabels=words,\n",
    "    fmt=\".3f\",\n",
    "    linewidths=0.5,\n",
    "    linecolor=\"gray\"\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Attention Weights Without Positional Encoding\")\n",
    "plt.xlabel(\"Key (Attended To)\")\n",
    "plt.ylabel(\"Query (Attending)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26686f58-a3ed-4571-a568-49ca5bb2e150",
   "metadata": {},
   "source": [
    "##### Results with position encoding\n",
    "- Each word attends most strongly to itself (diagonal values ~0.45).\n",
    "- Each word attends equally but weakly to the other words (off-diagonal values ~0.27)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b3e2ae-6c3c-44e4-bfef-0c109c10757f",
   "metadata": {},
   "source": [
    "#### Attention with with pos encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6203bf3-54dc-4f8a-b2de-5518d14da318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention with positional encoding\n",
    "Q_with_pos = K_with_pos = V_with_pos = combined\n",
    "output_with_pos, attention_weights_with_pos = scaled_dot_product_attention(Q_with_pos, K_with_pos, V_with_pos)\n",
    "\n",
    "print(\"\\nAttention Weights With Positional Encoding:\\n\", attention_weights_with_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d46a3f-7594-45e2-8bfb-0d2b75b99760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention weights with positional encoding (your example)\n",
    "attention_weights = attention_weights_with_pos[0]\n",
    "\n",
    "# Labels for the words\n",
    "words = [\"I\", \"love\", \"dogs\"]\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(\n",
    "    attention_weights,\n",
    "    annot=True,\n",
    "    cmap=\"YlGnBu\",\n",
    "    xticklabels=words,\n",
    "    yticklabels=words,\n",
    "    fmt=\".3f\",\n",
    "    linewidths=0.5,\n",
    "    linecolor=\"gray\"\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Attention Weights With Positional Encoding\")\n",
    "plt.xlabel(\"Key (Attended To)\")\n",
    "plt.ylabel(\"Query (Attending)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a12f89-a498-441b-a5a5-8fa01189d0c6",
   "metadata": {},
   "source": [
    "#### Why Does Positional Encoding Change the Attention?\n",
    "\n",
    "1. \"love\" attends more to \"dogs\" because of the phrase structure (\"love dogs\").\n",
    "2. \"dogs\" attends strongly to itself, as it is the object of the sentence.\n",
    "3. \"I\" attends to both itself and \"dogs\", possibly because \"I\" is the subject and \"dogs\" is the object.\n",
    "\n",
    "Without positional encoding, the attention weights were symmetric and each word attended mostly to itself.\n",
    "\n",
    "With positional encoding, the model learns to attend to words based on their position and semantic role in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58cc82b-1544-4a1d-8254-df4f99d82f67",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82457d39-9afa-4960-9399-6cfe77982d2f",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention for \"I love dogs\"\n",
    "\n",
    "- Instead of computing attention once, we split the Q, K, and V matrices into multiple \"heads\" (e.g., 2 or 8).\n",
    "- Each head computes its own attention weights, allowing the model to capture diverse patterns.\n",
    "- The outputs of all heads are concatenated and projected back to the original dimension.\n",
    "\n",
    "We shall use 2 heads:\n",
    "\n",
    "Each head will show different attention patterns, demonstrating how multi-head attention enriches the model's understanding of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81523292-ae3b-44f1-b1c2-986997fc9d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us rewrite the functions for multi-head\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    matmul_qk = np.matmul(Q, K.transpose(0, 2, 1))\n",
    "    dk = K.shape[-1]\n",
    "    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
    "    attention_weights = softmax(scaled_attention_logits)\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "# Embeddings (3 words, 4 dimensions)\n",
    "embeddings = np.array([\n",
    "    [1, 0, 0, 0],  # I\n",
    "    [0, 1, 0, 0],  # love\n",
    "    [0, 0, 1, 0],  # dogs\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Positional encoding\n",
    "positional_encoding = np.array([\n",
    "    [0, 0, 0, 0],      # Position 0\n",
    "    [0.5, 0.5, 0.5, 0.5],  # Position 1\n",
    "    [1, 1, 1, 1],      # Position 2\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Combined input (add batch dimension)\n",
    "combined = (embeddings + positional_encoding)[np.newaxis, :, :]  # Shape: (1, 3, 4)\n",
    "\n",
    "# Number of heads and dimensions per head\n",
    "num_heads = 2\n",
    "d_model = 4\n",
    "depth = d_model // num_heads  # 2 dimensions per head\n",
    "\n",
    "# Random projection matrices for Q, K, V\n",
    "W_Q = np.random.randn(d_model, d_model)\n",
    "W_K = np.random.randn(d_model, d_model)\n",
    "W_V = np.random.randn(d_model, d_model)\n",
    "\n",
    "# Project to Q, K, V\n",
    "Q = np.matmul(combined, W_Q)  # Shape: (1, 3, 4)\n",
    "K = np.matmul(combined, W_K)\n",
    "V = np.matmul(combined, W_V)\n",
    "\n",
    "# Split into heads\n",
    "def split_heads(x, num_heads):\n",
    "    batch_size, seq_len, d_model = x.shape\n",
    "    x = x.reshape(batch_size, seq_len, num_heads, depth)\n",
    "    return np.transpose(x, (0, 2, 1, 3))  # Shape: (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "def combine_heads(x):\n",
    "    batch_size, _, seq_len, _ = x.shape\n",
    "    x = np.transpose(x, (0, 2, 1, 3))  # Shape: (batch_size, seq_len, num_heads, depth)\n",
    "    return x.reshape(batch_size, seq_len, -1)  # Concatenate heads\n",
    "\n",
    "# Split into heads\n",
    "Q_heads = split_heads(Q, num_heads)  # Shape: (1, 2, 3, 2)\n",
    "K_heads = split_heads(K, num_heads)\n",
    "V_heads = split_heads(V, num_heads)\n",
    "\n",
    "# Compute attention for each head\n",
    "attention_outputs = []\n",
    "attention_weights_all = []\n",
    "\n",
    "for i in range(num_heads):\n",
    "    Q_head = Q_heads[0, i]  # Shape: (3, 2)\n",
    "    K_head = K_heads[0, i]\n",
    "    V_head = V_heads[0, i]\n",
    "\n",
    "    # Add batch dimension\n",
    "    Q_head = Q_head[np.newaxis, :, :]  # Shape: (1, 3, 2)\n",
    "    K_head = K_head[np.newaxis, :, :]\n",
    "    V_head = V_head[np.newaxis, :, :]\n",
    "\n",
    "    output, attention_weights = scaled_dot_product_attention(Q_head, K_head, V_head)\n",
    "    attention_outputs.append(output)\n",
    "    attention_weights_all.append(attention_weights)\n",
    "\n",
    "# Concatenate outputs from all heads\n",
    "attention_output = np.concatenate(attention_outputs, axis=-1)  # Shape: (1, 3, 4)\n",
    "\n",
    "# Print attention weights for each head\n",
    "for i, weights in enumerate(attention_weights_all):\n",
    "    print(f\"Attention Weights for Head {i+1}:\\n\", weights[0])\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    sns.heatmap(weights[0], annot=True, cmap=\"YlGnBu\", xticklabels=[\"I\", \"love\", \"dogs\"], yticklabels=[\"I\", \"love\", \"dogs\"])\n",
    "    plt.title(f\"Attention Head {i+1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37889049-8064-439e-a3a9-0e41fea0b4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_torch",
   "language": "python",
   "name": "py312_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
