{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b9189c-a9cf-40c1-86c7-4be0855e7834",
   "metadata": {},
   "source": [
    "### Examples to demonstrate the use of Pipelines from HuggingFace ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d27fcd6-cfb9-41c3-8b97-01d1420b1164",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raoji\\miniconda3\\envs\\py312_torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e040da3-3591-44cb-9fef-5cfa72fe6145",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fill \"Mask\" task ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfaf032a-bc42-441f-b923-76df594967c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.6457508206367493, 'token': 2156, 'token_str': 'see', 'sequence': 'i have to wake up in the morning and see a doctor'}\n",
      "{'score': 0.17833824455738068, 'token': 2655, 'token_str': 'call', 'sequence': 'i have to wake up in the morning and call a doctor'}\n",
      "{'score': 0.07508159428834915, 'token': 2424, 'token_str': 'find', 'sequence': 'i have to wake up in the morning and find a doctor'}\n",
      "{'score': 0.056827399879693985, 'token': 2131, 'token_str': 'get', 'sequence': 'i have to wake up in the morning and get a doctor'}\n",
      "{'score': 0.006895780097693205, 'token': 2022, 'token_str': 'be', 'sequence': 'i have to wake up in the morning and be a doctor'}\n"
     ]
    }
   ],
   "source": [
    "# Specifying the pipeline\n",
    "bert_unmasker = pipeline('fill-mask', model=\"bert-base-uncased\")\n",
    "text = \"I have to wake up in the morning and [MASK] a doctor\"\n",
    "result = bert_unmasker(text)\n",
    "for r in result:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5938936-457c-4279-8d28-537e81932558",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598046541213989}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment analysis example\n",
    "\n",
    "# By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. # # The model is downloaded and cached when you create the classifier object. \n",
    "# If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b39b99-c957-498b-b783-26c053ab9205",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598046541213989},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can even pass several sentences\n",
    "\n",
    "classifier(\n",
    "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfc57b-e468-4a25-8e96-3a195d7576ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Zero-shot classification - Task where we need to classify texts that haven’t been labelled ####\n",
    "\n",
    " * This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. \n",
    " \n",
    " * For this use case, the zero-shot-classification pipeline is very powerful: it allows you to specify which labels to use for the classification, so you don’t have to rely on the labels of the pretrained model.  \n",
    " \n",
    " * You’ve already seen how the model can classify a sentence as positive or negative using those two labels — but it can also classify the text using any other set of labels you like.\n",
    " \n",
    " * This pipeline is called zero-shot because you don’t need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694d9531-7b69-48b9-80e2-6ff17d69a697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This is a course about the Transformers library', 'labels': ['education', 'business', 'politics'], 'scores': [0.8445959687232971, 0.1119764968752861, 0.043427541851997375]}\n",
      "{'sequence': 'The movie was very good', 'labels': ['positive', 'neutral', 'negative'], 'scores': [0.9884912967681885, 0.007404666393995285, 0.004104042425751686]}\n"
     ]
    }
   ],
   "source": [
    "# Zero shot classification\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "print(classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    "))\n",
    "\n",
    "print(classifier(\n",
    "    \"The movie was very good\",\n",
    "    candidate_labels = [\"positive\",\"negative\",\"neutral\"]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd1d04-5836-4e1e-ae61-1686c44dbba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Text generation ####\n",
    "\n",
    "* The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. \n",
    "\n",
    "* This is similar to the predictive text feature that is found on many phones. \n",
    "\n",
    "* Text generation involves randomness, so it’s normal if you don’t get the same results as shown below.\n",
    "\n",
    "* You can control how many different sequences are generated with the argument num_return_sequences and the total length of the output text with the argument max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08c074f6-2121-49bf-9579-a3b2dac80ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"In this course, we will teach you how to create a simple and user-friendly application using React.\\n\\nThere is a ton of good content on React.js, so I recommend you to read it in order to learn about the concepts and how to use them.\\n\\nNow let's get started…\\n\\nStep One\\n\\nStep Two\\n\\nStep Three\\n\\nStep Four\\n\\nStep Five\\n\\nStep Six\\n\\nStep Seven\\n\\nStep Eight\\n\\nStep Nine\\n\\nStep Ten\\n\\nStep Eleven\\n\\nStep Twelve\\n\\nStep 13\\n\\nStep 14\\n\\nStep 15\\n\\nStep 16\\n\\nStep 17\\n\\nStep 18\\n\\nStep 19\\n\\nStep 20\\n\\nStep 21\\n\\nStep 22\\n\\nStep 23\\n\\nStep 24\\n\\nStep 25\\n\\nStep 26\\n\\nStep 27\\n\\nStep 28\\n\\nStep 29\\n\\nStep 30\\n\\nStep 31\\n\\nStep 32\\n\\nStep 33\\n\\nStep 34\\n\\nStep 35\\n\\nStep 36\\n\\nStep 37\\n\\nStep 38\\n\\nStep 39\\n\\nStep 40\\n\\nStep 41\\n\\nStep 42\\n\\nStep 43\\n\\nStep 44\\n\\nStep 45\\n\\nStep 46\\n\\nStep 47\\n\\nStep 48\\n\\nStep 49\\n\\nStep 50\\n\\nStep\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Generation\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a22429d-4957-45e7-a2b1-b80809ee31b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Using any model from the Hub in a pipeline ####\n",
    "\n",
    "* The previous examples used the default model for the task at hand, but you can also choose a particular model from the Hub to use in a pipeline for a specific task — say, text generation. \n",
    "\n",
    "* Go to the Model Hub and click on the corresponding tag on the left to display only the supported models for that task.<br>\n",
    "https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ae351aa-97ff-46f1-9d68-d2f304bf6048",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use these techniques to create the most efficient, and efficient way to accomplish your goal.\\n\\n\\n\\n\\nIf you want to see the videos you are going to need to see a video on the new video, click here.\\nAnd in the meantime, you can use this tutorial to help you improve your workflow and performance for yourself. Be sure to check out the main video and videos by clicking the link below to see some more tutorials.\\nThe video is now available on YouTube.\\nIf you want to view the videos in the video, click here.'},\n",
       " {'generated_text': 'In this course, we will teach you how to create a simple video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how to create a video that shows you how'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ceb3af-fbed-4478-a269-c6c4fde48be2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Named entity recognition ####\n",
    "\n",
    "Named entity recognition (NER) is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c58cc0c6-b8a0-488e-b305-0bc2d87c3717",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9981694),\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.9796019),\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9932106),\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1e2ec-e1c2-4fef-a271-315b975e8276",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Summarization ####\n",
    "\n",
    "Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7bbc4f0-8303-4455-bbae-bf371ac4ab49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \" Europe is a peninsula of the Eurasian supercontinent . It is bordered by the Arctic Ocean to the north, the Atlantic to the west, and the Mediterranean, Black, and Caspian seas to the south . Europe's main peninsulas are the Iberian, Italian, and Balkan, located in southern Europe .\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    Europe is the second-smallest continent. The name Europe, or Europa, is believed to be of Greek origin, as it is the name of a princess in Greek mythology. The name Europe may also come from combining the Greek roots eur- (wide) and -op (seeing) to form the phrase “wide-gazing.”\n",
    "\n",
    "Europe is often described as a “peninsula of peninsulas.” A peninsula is a piece of land surrounded by water on three sides. Europe is a peninsula of the Eurasian supercontinent and is bordered by the Arctic Ocean to the north, the Atlantic Ocean to the west, and the Mediterranean, Black, and Caspian seas to the south.\n",
    "\n",
    "Europe’s main peninsulas are the Iberian, Italian, and Balkan, located in southern Europe, and the Scandinavian and Jutland, located in northern Europe. The link between these peninsulas has made Europe a dominant economic, social, and cultural force throughout recorded history.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d694d",
   "metadata": {},
   "source": [
    "#### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47219e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([1, 7, 768])\n",
      "Embeddings:\n",
      "tensor([[[-0.4095,  0.1067, -0.3270,  ..., -0.5877, -0.2818,  0.2980],\n",
      "         [ 0.2982,  0.3689, -0.4391,  ..., -0.4908,  0.2701,  0.0518],\n",
      "         [ 0.1266, -0.3955, -0.0369,  ..., -0.5126,  0.2784,  0.0832],\n",
      "         ...,\n",
      "         [-0.4505,  0.0647, -0.3500,  ..., -0.5077, -0.3530, -0.1903],\n",
      "         [-0.1960,  0.1788,  0.0403,  ..., -0.0181, -0.3388, -0.5101],\n",
      "         [ 0.7388,  0.1887, -0.5517,  ...,  0.0199, -0.5753, -0.2244]]])\n"
     ]
    }
   ],
   "source": [
    "emb = pipeline(\"feature-extraction\", model=\"google-bert/bert-base-uncased\")\n",
    "X = emb(\"industrial anomaly detection with audio\", return_tensors=True)  # [1, seq_len, hidden]\n",
    "print(f\"Embeddings shape: {X.size()}\\nEmbeddings:\\n{X}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a842a58b-9c87-457f-8c5f-8ca13b5e8f95",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Translation ####\n",
    "\n",
    "* For translation, you can use a default model if you provide a language pair in the task name (such as \"translation_en_to_fr\"), \n",
    "\n",
    "* but the easiest way is to pick the model you want to use on the Model Hub. \n",
    "\n",
    "* Here we’ll try translating from French to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aa8cafc-1e44-4e3c-944d-3d21ebae2d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Hallo Welt, dies ist ein Beispiel für die Übersetzung mit Hugging Face Pipelines.'}]\n"
     ]
    }
   ],
   "source": [
    "# Create a translation pipeline for English to German\n",
    "# The 'translation_en_to_de' task uses a default model suitable for this pair.\n",
    "translator = pipeline(\"translation_en_to_de\")\n",
    "\n",
    "# Define the text to be translated\n",
    "text_to_translate = \"Hello world! This is an example of translation using Hugging Face pipelines.\"\n",
    "\n",
    "# Perform the translation\n",
    "translated_text = translator(text_to_translate)\n",
    "\n",
    "# Print the result\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a45960",
   "metadata": {},
   "source": [
    "#### Q&A with pipeline(\"question-answering\")\n",
    "What it does\n",
    "\n",
    "Extracts an answer span from a given context for a question. (This is extractive QA—answers come from the context text.)\n",
    "\n",
    "Key knobs you’ll actually use\n",
    "\n",
    "- Model choice: pick a SQuAD/SQuAD2-finetuned checkpoint (e.g., deepset/roberta-base-squad2, distilbert-base-cased-distilled-squad).\n",
    "\n",
    "- Device & precision: device=0 (single GPU) or device_map=\"auto\" (multi-GPU/CPU offload).\n",
    "\n",
    "- Top-k candidates: top_k returns N best spans with scores.\n",
    "\n",
    "- Unanswerable questions: handle_impossible_answer=True (only for SQuAD2-style models).\n",
    "\n",
    "- Answer length: max_answer_len limits span length (useful to prevent rambling answers).\n",
    "\n",
    "- Long contexts: max_seq_len and doc_stride let the pipeline window over long passages.\n",
    "\n",
    "Input / Output shape\n",
    "\n",
    "- Input: {\"question\": \"...\", \"context\": \"...\"} or a list of such dicts (for batching).\n",
    "\n",
    "- Output: dict (or list) with answer, score, start, end. With top_k>1, returns a list of candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b84edc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.30193451046943665, 'start': 4, 'end': 8, 'answer': 'UR5e'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = pipeline(\n",
    "    task=\"question-answering\",\n",
    "    model=\"deepset/roberta-base-squad2\",   # SQuAD2 (can predict “no answer”)\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "context = \"\"\"The UR5e is a collaborative robot arm from Universal Robots.\n",
    "It supports force-torque sensing and is commonly used in education and industry.\"\"\"\n",
    "question = \"What is used a lot in education?\"\n",
    "\n",
    "qa(question=question, context=context)\n",
    "# -> {'score': ..., 'start': ..., 'end': ..., 'answer': 'UR5e'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1b012",
   "metadata": {},
   "source": [
    "#### Use the pipeline class effectively (battle-tested tips)\n",
    "\n",
    "1. Pick the right checkpoint for the task (e.g., a QA-finetuned model for extractive QA). Pipeline auto-routes inputs/outputs per task. \n",
    "\n",
    "\n",
    "2. Control outputs explicitly:\n",
    "\n",
    "- Classification confidence curves → return_all_scores=True.\n",
    "\n",
    "- “Only completions, not the prompt” in generation → return_full_text=False. \n",
    "\n",
    "\n",
    "3. Batch properly: Feed a datasets.Dataset + KeyDataset to stream and batch on GPU—cleaner and typically as fast as manual loops. Tune batch_size only if you see under-utilization. \n",
    "\n",
    "\n",
    "4. Mind sequence lengths: For tasks that support it, use padding/truncation (or task-specific strategies) to avoid over-length errors. Check each pipeline’s doc for which kwargs are honored. \n",
    "\n",
    "\n",
    "5. Performance & memory: Use device_map=\"auto\" + accelerate and torch_dtype=\"auto\" to shrink memory and speed up inference on modern hardware. \n",
    "\n",
    "\n",
    "6. When you need raw features (embeddings for clustering, etc.), use the feature-extraction pipeline with return_tensors=True and pool as needed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b263a-99c4-45c4-a858-70f8d5cf97d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
