{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d5b524-d381-484e-99a5-c1974227d720",
   "metadata": {},
   "source": [
    "### Example - ViT for image classification\n",
    "\n",
    "Why resize to 224×224? \n",
    "\n",
    "ViT was pretrained at 224; matching size & normalization speeds convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "422150b2-4577-40f5-8289-34c5388510cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification, TrainingArguments, Trainer, DefaultDataCollator\n",
    "from torchvision.transforms import Compose, Resize, RandomHorizontalFlip, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import numpy as np, evaluate, torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45184552-413b-448f-b6e2-88ea33d2d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data + labels\n",
    "ds = load_dataset(\"cifar10\")\n",
    "id2label = {i: n for i, n in enumerate(ds[\"train\"].features[\"label\"].names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d01cf7af-0487-4ec6-b3b8-36a4664e7b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Fetching 1 files: 100%|█████████████████████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Map: 100%|███████████████████████| 50000/50000 [06:05<00:00, 136.78 examples/s]\n",
      "Map: 100%|███████████████████████| 10000/10000 [01:22<00:00, 121.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Image processor (does resize to 224 + normalization automatically for ViT)\n",
    "proc = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# Preprocess with batched map\n",
    "def preprocess(examples):\n",
    "    # Ensure everything is PIL or arrays; processor can handle both\n",
    "    images = [\n",
    "        img if isinstance(img, Image.Image) else Image.fromarray(img)\n",
    "        for img in examples[\"img\"]\n",
    "    ]\n",
    "    out = proc(images=images)  # don't set return_tensors; datasets will tensorize later\n",
    "    return {\"pixel_values\": out[\"pixel_values\"], \"labels\": examples[\"label\"]}\n",
    "\n",
    "train_ds = ds[\"train\"].map(preprocess, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "test_ds  = ds[\"test\"].map(preprocess,  batched=True, remove_columns=ds[\"test\"].column_names)\n",
    "\n",
    "# Convert to torch tensors for Trainer\n",
    "train_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])\n",
    "test_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5bae6b8-8b00-4b49-89cf-5222cd53108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model (pretrained ViT, new 10-class head)\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=10, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Baseline: freeze the whole ViT encoder, train only the classification head\n",
    "for p in model.vit.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69866f9a-7b12-4c9e-a1ce-8313dd765aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(p):  # tiny helper\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30fe188b-c804-4cb5-874d-a25c1701ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments inputs\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"vit-cifar10\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    remove_unused_columns=False,              # If there are any()\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ee1fce2-3eea-466b-8720-39abd3c8ada5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 18:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.639600</td>\n",
       "      <td>1.649166</td>\n",
       "      <td>0.930700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 03:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6491659879684448, 'eval_accuracy': 0.9307, 'eval_runtime': 188.9428, 'eval_samples_per_second': 52.926, 'eval_steps_per_second': 0.831, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# TRainer class instantiation and train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37657efa-f0c5-4848-a9ff-cc90b09a737b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: cat | True: cat\n"
     ]
    }
   ],
   "source": [
    "# One-image inference demo\n",
    "ex = test_ds[0]  # already tensors\n",
    "with torch.no_grad():\n",
    "    out = model(ex[\"pixel_values\"].unsqueeze(0).to(model.device))\n",
    "    print(\"Pred:\", id2label[out.logits.argmax(-1).item()], \"| True:\", id2label[ex[\"labels\"].item()])\n",
    "\n",
    "# 7) Save\n",
    "trainer.save_model(\"vit-cifar10/best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44fd13cf-1964-45d7-9de3-0d34ef2e4553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vit-cifar10/best\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you initialized the image processr as part of training it would get saved else\n",
    "# save it seapartely\n",
    "\n",
    "trainer.save_model(\"vit-cifar10/best\")\n",
    "proc.save_pretrained(\"vit-cifar10/best\")  # writes preprocessor_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60b44831-3a1f-487d-a08a-790bfd990a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane\n"
     ]
    }
   ],
   "source": [
    "# Predict function\n",
    "def predict(image_path, model_dir=\"vit-cifar10/best\"):\n",
    "    # 1) Load model + processor\n",
    "    processor = AutoImageProcessor.from_pretrained(model_dir)\n",
    "    model = ViTForImageClassification.from_pretrained(model_dir)\n",
    "    model.eval()\n",
    "\n",
    "    # 2) Load + preprocess image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=img, return_tensors=\"pt\")\n",
    "\n",
    "    # 3) Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred_id = outputs.logits.argmax(-1).item()\n",
    "\n",
    "    # 4) Map id → label\n",
    "    return model.config.id2label[pred_id]\n",
    "\n",
    "# Call function\n",
    "print(predict(\"plane.jpg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ad11b-45fa-46a9-a003-407b756bc6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_torch",
   "language": "python",
   "name": "py312_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
