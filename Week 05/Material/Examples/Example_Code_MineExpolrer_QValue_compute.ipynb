{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c19458c5",
   "metadata": {},
   "source": [
    "### Computing Q Vaues using Bellman's updated equation ###\n",
    "\n",
    "Bellmann Equation is: $Q(s,a) = Q(s,a) + \\alpha (R_s + \\gamma * max(Q(s',a') - Q(s,a))$\n",
    "\n",
    "where $Q(s,a)$ is the $Q$ value of the state it is in.\n",
    "\n",
    "#### The method for training an agent using Q-Learning is done as follows: ####\n",
    "    - In the start state the agent can take any random action to get to the next state\n",
    "    - From this state the Agent takes the optimal path as per the Bellman equation till end of episode.\n",
    "    - When the episode is over a new episode starts but the Q-Values are carried over to the next episode\n",
    "    - Many episodes are iteratively run till the Q-Values of any of the state, action pairs do not change (Optimal policy)\n",
    "    - The Q-Values are populated in a Q-Table with columns as actions and rows as states.\n",
    "\n",
    "For the MineExplorer Exercise and to make this process intuitively clear, I have shown the sample calculation for 2 episodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32acbcb7",
   "metadata": {},
   "source": [
    "1. Keep in mind the MineExplorer has 6 states with 2 of them being terminal states.\n",
    "2. The terminal state 1 gives a reward of 100 and the terminal state 6 gives a reward of 40.\n",
    "3. All other states give a reward of 0.\n",
    "4. Mine explorer has two actions - move left (0) or move right (1).\n",
    "5. I will take the gamma as in the class example (0.5) and will assume alpha = 1 (you will use values given in the exercise).\n",
    "6. Let the start state always be 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6425356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef7d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table at start\n",
      "[[100.   0.]\n",
      " [  0.   0.]\n",
      " [  0.   0.]\n",
      " [  0.   0.]\n",
      " [  0.   0.]\n",
      " [  0.  40.]]\n",
      "0\n",
      "Q Values after Episode 1:\n",
      "[[100.   0.]\n",
      " [ 50.   0.]\n",
      " [  0.   0.]\n",
      " [  0.   0.]\n",
      " [  0.   0.]\n",
      " [  0.  40.]]\n",
      "\n",
      "Q Values after Episode 2:\n",
      "[[100.   0.]\n",
      " [ 50.   0.]\n",
      " [ 25.   0.]\n",
      " [  0.   0.]\n",
      " [  0.   0.]\n",
      " [  0.  40.]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Q-Table\n",
    "Q = np.zeros((6,2)) # 1st column represents action 'left' and 2nd column represents action 'right'\n",
    "\n",
    "alpha = 1.  # Learningrate\n",
    "gamma = 0.5 # Discount factor\n",
    "\n",
    "# Populate the terminal states with rewards\n",
    "Q[0][0] = 100\n",
    "Q[5][1] = 40\n",
    "\n",
    "reward = [100,0,0,0,0,40] # The reward system\n",
    "action = [0,1]            # The action codes for 'left','right'\n",
    "\n",
    "print(f\"Q-Table at start\\n{Q}\")\n",
    "\n",
    "\n",
    "# Episode 1: \n",
    "# Remember python counts from 0\n",
    "# Starts at state 2 and let the agent make a random first action to the left\n",
    "# It then moves to the next state, which is 1.\n",
    "\n",
    "episode = True # Indication that episode is not done\n",
    "\n",
    "state = 2 # Set start state \n",
    "\n",
    "actions = action[0] # First random action \n",
    "\n",
    "# Compute Q value of each state till the episode ends\n",
    "while episode:\n",
    "    if actions == 0:  # Goes left\n",
    "        Q[state,action[0]] = Q[state,action[0]] + alpha * (reward[state] + gamma * np.max(Q[state-1]) - Q[state,action[0]] )\n",
    "    else:           # Goes right\n",
    "        Q[state,action[0]] = Q[state,action[0]] + alpha * (reward[state] + gamma * np.max(Q[state+1]) - Q[state,action[0]] )\n",
    "\n",
    "    # the next state is 1\n",
    "    if actions == 0:\n",
    "        state = state - 1\n",
    "    else:\n",
    "        state = state + 1   \n",
    "    \n",
    "    if state == 0 or state == 5:\n",
    "        episode = False\n",
    "        \n",
    "print(f\"Q Values after Episode 1:\\n{Q}\")\n",
    "\n",
    "# Let us do a 2nd episode with statrt state = 2 (already set)\n",
    "# Let the agent take a random first action as right this time and then follow the optimal route\n",
    "# Compute Q value of each state till the episode ends\n",
    "# Note that the Q-Table remains as last updated in Episode 1\n",
    "\n",
    "episode = True # Indication that episode is not done\n",
    "\n",
    "state = 2 # Reset start state \n",
    "\n",
    "actions = action[1] # Random right step first\n",
    "\n",
    "while episode:\n",
    "    if actions == 0:  # Goes left\n",
    "        Q[state,action[0]] = Q[state,action[0]] + alpha * (reward[state] + gamma * np.max(Q[state-1]) - Q[state,action[0]] )\n",
    "    else:           # Goes right\n",
    "        Q[state,action[0]] = Q[state,action[0]] + alpha * (reward[state] + gamma * np.max(Q[state+1]) - Q[state,action[0]] )\n",
    "\n",
    "    # the next state is 1\n",
    "    if actions ==0:\n",
    "        state = state - 1\n",
    "    else:\n",
    "        state = state + 1\n",
    "    \n",
    "        \n",
    "    actions = action[0] # Then continue left as it is the optimal action   \n",
    "    \n",
    "    if state == 0 or state == 5:\n",
    "        episode = False\n",
    "        \n",
    "print(f\"\\nQ Values after Episode 2:\\n{Q}\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869eeeb5",
   "metadata": {},
   "source": [
    "### Please Note: ###\n",
    "\n",
    "1. This is just bare code to show how the equations are implemented and computation is done\n",
    "2. I have assumed what the random action will be wheras when you code the whole exercise you will have to generate that random action\n",
    "3. Make your code a combination of for loops for number of episodes and while loop for each episode - that is the most efficient way.\n",
    "4. Study and understand how I have implemented and computed the Bellman's equation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca318079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
