{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gymnasium ###\n",
    "\n",
    "https://gymnasium.farama.org/index.html\n",
    "\n",
    "Gymnasium is a project that provides an API for all single agent reinforcement learning environments, and includes implementations of common environments.\n",
    "\n",
    "The API contains four key functions: \n",
    "\n",
    "* make, reset, step and render, \n",
    "\n",
    "that this basic usage will introduce you to. \n",
    "\n",
    "At the core of Gymnasium is Env, a high-level python class representing a markov decision process (MDP) from reinforcement learning theory.\n",
    "\n",
    " Within gymnasium, environments (MDPs) are implemented as Env classes, along with Wrappers, which provide helpful utilities and can change the results passed to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u> Install packages: https://anaconda.org/search?q=gymnasium </u>\n",
    "\n",
    "__Before install gymnasium Must install: (for Windows users)__\n",
    "conda install swig\n",
    "Microsoft C++ build tools - https://visualstudio.microsoft.com/downloads/\n",
    "https://www.youtube.com/watch?v=gMgj4pSHLww\n",
    "\n",
    "After that:\n",
    "\n",
    " conda install conda-forge::gymnasium-all\n",
    "\n",
    "OR individually\n",
    "\n",
    "conda install -c conda-forge gymnasium \n",
    "conda install -c conda-forge gymnasium-box2d \n",
    "conda install -c conda-forge gymnasium-classic_control \n",
    "\n",
    "OR\n",
    "\n",
    "pip install gymnasium<br>\n",
    "pip install \"gymnasium[box2d]\n",
    "\n",
    "and so on... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Environments ####\n",
    "\n",
    "Initializing environments is very easy in Gymnasium and can be done via the make function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import gymnasium as gym\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return an Env for users to interact with. \n",
    "\n",
    "To see all environments you can create, use __gymnasium.envs.registry.keys()__.make includes a number of additional parameters to adding wrappers, specifying keywords to the environment and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.envs.registry.keys()\n",
    "\n",
    "# for i,e in enumerate(envs):\n",
    "#     print(f\"Environment {i}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interacting with the Environment ####\n",
    "\n",
    "The classic “agent-environment loop” pictured below is simplified representation of reinforcement learning that Gymnasium implements.\n",
    "\n",
    "<img src =\"blackjack_AE_loop.jpg\" style=\"height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop is implemented using the following gymnasium code - here is an example of a Lunar lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, an environment is created using make with an additional keyword \"render_mode\" that specifies how the environment should be visualised. \n",
    "\n",
    "* After initializing the environment, we reset the environment to get the first observation of the environment. \n",
    "* Next, the agent performs an action in the environment, step, this can be imagined as moving a robot or pressing a button on a games’ controller that causes a change within the environment. \n",
    "* As a result, the agent receives a new observation from the updated environment along with a reward for taking the action. \n",
    "* This reward could be for instance positive for destroying an enemy or a negative reward for moving into lava. One such action-observation exchange is referred to as a timestep.\n",
    "* However, after some timesteps, the environment may end, this is called the terminal state. For instance, the robot may have crashed, or the agent have succeeded in completing a task, the environment will need to stop as the agent cannot continue. \n",
    "* In gymnasium, if the environment has terminated, this is returned by step. Similarly, we may also want the environment to end after a fixed number of timesteps, in this case, the environment issues a truncated signal. \n",
    "* If either of terminated or truncated are true then reset should be called next to restart the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action and observation spaces ####\n",
    "\n",
    "Every environment specifies the format of valid actions and observations with the env.action_space and env.observation_space attributes. This is helpful for both knowing the expected input and output of the environment as all valid actions and observation should be contained with the respective space.\n",
    "\n",
    "In the example, we sampled random actions via env.action_space.sample() instead of using an agent policy, mapping observations to actions which users will want to make.\n",
    "\n",
    "Every environment should have the attributes action_space and observation_space, both of which should be instances of classes that inherit from Space. Gymnasium has support for a majority of possible spaces users might need:\n",
    "\n",
    "    Box: describes an n-dimensional continuous space. It’s a bounded space where we can define the upper and lower limits which describe the valid values our observations can take.\n",
    "\n",
    "    Discrete: describes a discrete space where {0, 1, …, n-1} are the possible values our observation or action can take. Values can be shifted to {a, a+1, …, a+n-1} using an optional argument.\n",
    "\n",
    "    Dict: represents a dictionary of simple spaces.\n",
    "\n",
    "    Tuple: represents a tuple of simple spaces.\n",
    "\n",
    "    MultiBinary: creates an n-shape binary space. Argument n can be a number or a list of numbers.\n",
    "\n",
    "    MultiDiscrete: consists of a series of Discrete action spaces with a different number of actions in each element.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation space\n",
    "print(f\"For the environment: Cartpole, the Shape of the Observation space: {env.observation_space.shape}\\n\")\n",
    "# Let us sample this\n",
    "print(f\"Observations space sample:\\n{env.observation_space.sample()}\\n\")\n",
    "\n",
    "# Action space\n",
    "print(f\"The action space: {env.action_space}\\n\")\n",
    "# Sample action space\n",
    "print(f\"Actions: {env.action_space.sample()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see what we get when the agent takes an action in an environment\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "print(f\"Reset to a start position: {env.reset()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us take an action in this environment\n",
    "print(f\"Result of action: {env.step(1)}\")\n",
    "\n",
    "# Let us store this information\n",
    "\n",
    "obs, reward, terminated, truncated, info = env.step(1)\n",
    "\n",
    "print(f\"\\n\\nOutput of action by an agent is:\\n\\nobservation[next state]: {obs}\\nReward: {reward}\\nTerminated or end of episode (True or False): {terminated}\\nTruncated (min runs): {truncated}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
