{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceac6bb5",
   "metadata": {},
   "source": [
    "#### SVD in NLP\n",
    "\n",
    "In text mining, a collection of documents can be represented as a term-document matrix. \n",
    "\n",
    "    - Each row represents a term (a word), and each column represents a document. - An entry in the matrix might be the frequency of a term in a document. \n",
    "    - This is the foundation of Latent Semantic Analysis (LSA). \n",
    "    \n",
    "- Original Data Matrix (A): A matrix where rows are terms and columns are documents.\n",
    "- Left Singular Vectors (columns of U): These vectors represent the patterns of how words group together across documents. A vector with high values for terms like \"programming,\" \"software,\" and \"code\" would correspond to a \"technology\" topic.\n",
    "- Right Singular Vectors (columns of V): These vectors represent how documents cluster around different topics. A document with a high score for the \"technology\" singular vector is likely a document about technology.   \n",
    "- Singular Values ($\\Sigma$): The largest singular values indicate the most important topics, while smaller singular values are associated with less common themes or noise. \n",
    "\n",
    "What happens with truncation?\n",
    "\n",
    "- By performing a truncated SVD and keeping only the top k singular vectors, you reduce the dimensionality of the data. \n",
    "- The original term-document matrix, which might be sparse and noisy, is approximated by a lower-rank matrix that captures only the major underlying topics. \n",
    "- This compact representation is useful for tasks like: \n",
    "    - Topic discovery: The singular vectors directly reveal the main topics in the document collection.\n",
    "    - Document similarity: Documents can be compared based on their scores for these main topics rather than individual word counts, revealing deeper semantic relationships. \n",
    "\n",
    "Example\n",
    "\n",
    "Latent Semantic Analysis (LSA) on a tiny term–document matrix. \n",
    "\n",
    "It shows how SVD discovers hidden topics and smooths synonymy—even with a handful of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb4c54f",
   "metadata": {},
   "source": [
    "Mini corpus\n",
    "\n",
    "d1: “human computer interaction”\n",
    "\n",
    "d2: “user interface computer”\n",
    "\n",
    "d3: “graph theory trees”\n",
    "\n",
    "d4: “graph minors survey”\n",
    "\n",
    "-- Terms (rows): human, user, computer, interface, graph, tree(s)\n",
    "\n",
    "-- Docs (cols): d1–d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638dabdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1 similarity: 1.000\n",
      "d2 similarity: 1.000\n",
      "d3 similarity: 0.000\n",
      "d4 similarity: 0.000\n",
      "\n",
      "Term embeddings (2D):\n",
      " [[-0.707  0.   ]\n",
      " [-0.707  0.   ]\n",
      " [-1.414  0.   ]\n",
      " [-1.414  0.   ]\n",
      " [ 0.    -1.376]\n",
      " [ 0.    -0.851]]\n",
      "\n",
      "Doc embeddings (2D):\n",
      " [[-1.581 -1.581  0.     0.   ]\n",
      " [ 0.     0.    -1.376 -0.851]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# term order: human, user, computer, interface, graph, trees\n",
    "# docs: d1, d2, d3, d4\n",
    "X = np.array([\n",
    "    [1, 0, 0, 0],  # human\n",
    "    [0, 1, 0, 0],  # user\n",
    "    [1, 1, 0, 0],  # computer\n",
    "    [1, 1, 0, 0],  # interface\n",
    "    [0, 0, 1, 1],  # graph\n",
    "    [0, 0, 1, 0],  # trees (singularizing to 'tree' not necessary for the demo)\n",
    "], dtype=float)\n",
    "\n",
    "# 1) SVD\n",
    "U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "# Keep k=2 latent dimensions\n",
    "k = 2\n",
    "Uk, Sk, Vtk = U[:, :k], np.diag(S[:k]), Vt[:k, :]\n",
    "\n",
    "# 2) Low-rank embeddings\n",
    "# term embeddings: Uk * Sk     (6×2)\n",
    "term_emb = Uk @ Sk\n",
    "\n",
    "# doc embeddings:  Sk * Vtk^T  (2×4)\n",
    "doc_emb = Sk @ Vtk\n",
    "\n",
    "# 3) Rank docs for a query: \"human computer\"\n",
    "# Make a term vector in original space and project to latent space\n",
    "terms = [\"human\",\"user\",\"computer\",\"interface\",\"graph\",\"trees\"]\n",
    "q = np.zeros(len(terms))\n",
    "for t in [\"human\",\"computer\"]:\n",
    "    q[terms.index(t)] = 1.0\n",
    "\n",
    "# project query: q_k = q^T U_k S_k^{-1}\n",
    "q_k = (q @ Uk) @ np.linalg.inv(Sk)  # 1×2\n",
    "\n",
    "# cosine similarity between query (2D) and each doc (2×4 columns)\n",
    "def cos_sim(a, B):\n",
    "    # a shape: (2,), B shape: (2,n)\n",
    "    num = (a[:,None] * B).sum(axis=0)\n",
    "    den = np.linalg.norm(a) * np.linalg.norm(B, axis=0)\n",
    "    return num / (den + 1e-12)\n",
    "\n",
    "sims = cos_sim(q_k.ravel(), doc_emb)\n",
    "for i, s in enumerate(sims, start=1):\n",
    "    print(f\"d{i} similarity: {s:.3f}\")\n",
    "\n",
    "# Also show which terms are “near” which docs in 2D (optional)\n",
    "print(\"\\nTerm embeddings (2D):\\n\", term_emb.round(3))\n",
    "print(\"\\nDoc embeddings (2D):\\n\", doc_emb.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db53f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: human computer\n",
      "doc1 score=1.000  -> human computer interaction\n",
      "doc2 score=0.962  -> user interface for computer applications\n",
      "doc4 score=0.063  -> survey of graph minors and applications\n",
      "doc3 score=-0.214  -> graph theory and tree structures\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "docs = [\n",
    "    \"human computer interaction\",\n",
    "    \"user interface for computer applications\",\n",
    "    \"graph theory and tree structures\",\n",
    "    \"survey of graph minors and applications\"\n",
    "]\n",
    "\n",
    "# 1) TF-IDF -> 2) TruncatedSVD (k=2) -> 3) L2-normalize\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "svd = TruncatedSVD(n_components=2, random_state=0)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(vectorizer, svd, normalizer)\n",
    "\n",
    "doc_vecs = lsa.fit_transform(docs)  # shape: (n_docs, 2)\n",
    "\n",
    "print(doc_vecs)\n",
    "\n",
    "# Query and rank\n",
    "query = \"human computer\"\n",
    "q_vec = lsa.transform([query])      # shape: (1, 2)\n",
    "\n",
    "def cosine_sim(a, B):\n",
    "    # a: (1,d), B: (n,d), assume already L2-normalized by pipeline\n",
    "    return (B @ a.ravel())\n",
    "\n",
    "scores = cosine_sim(q_vec, doc_vecs)\n",
    "order = np.argsort(-scores)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "for i in order:\n",
    "    print(f\"doc{i+1} score={scores[i]:.3f}  -> {docs[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
