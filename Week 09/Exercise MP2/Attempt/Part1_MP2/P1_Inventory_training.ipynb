{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44873592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "\n",
    "# Import the environment and classes provided by the assignment\n",
    "from Inventory_env_class import InventoryManagementEnv, NormalizeObservation, ReplayBuffer, DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45190ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Device & environment setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Create and normalize the environment (the assignment told us to use NormalizeObservation)\n",
    "env = InventoryManagementEnv()\n",
    "env = NormalizeObservation(env)\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]   # should be 6\n",
    "n_actions = env.action_space.n             # should be 3\n",
    "\n",
    "print(\"Observation dim:\", obs_dim)\n",
    "print(\"Action count:\", n_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cebd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Hyperparameters (simple defaults you can tweak)\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Training hyperparams\n",
    "num_episodes = 5000         # start smaller (e.g. 2000) if using CPU\n",
    "max_steps_per_episode = 500 # env has a built-in max_steps value; we keep similar\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "lr = 1e-3\n",
    "replay_capacity = 20000\n",
    "\n",
    "# Epsilon-greedy\n",
    "eps_start = 1.0\n",
    "eps_end = 0.05\n",
    "eps_decay = 3000.0   # larger -> slower decay\n",
    "\n",
    "# Target network update frequency (in steps)\n",
    "target_update_every = 1000\n",
    "\n",
    "# For logging / plotting\n",
    "print_interval = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11898ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Initialize networks, optimizer, replay buffer\n",
    "policy_net = DQN(obs_dim, n_actions).to(device)\n",
    "target_net = DQN(obs_dim, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "replay_buffer = ReplayBuffer(capacity=replay_capacity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2812f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Helper functions - epsilon schedule, select_action, and optimize step\n",
    "\n",
    "def get_epsilon(it):\n",
    "    # smooth exponential-ish decay\n",
    "    return eps_end + (eps_start - eps_end) * math.exp(-1.0 * it / eps_decay)\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    # state: numpy array (observation)\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(n_actions)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            qvals = policy_net(t)\n",
    "            return int(qvals.argmax().cpu().numpy())\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "def optimize_model():\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    # convert to tensors\n",
    "    states_t = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "    actions_t = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
    "    rewards_t = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "    next_states_t = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "    dones_t = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "    # Q(s,a)\n",
    "    q_values = policy_net(states_t).gather(1, actions_t)\n",
    "\n",
    "    # target: r + gamma * max_a' Q_target(s', a') * (1 - done)\n",
    "    with torch.no_grad():\n",
    "        next_q = target_net(next_states_t).max(1)[0].unsqueeze(1)\n",
    "        target_q = rewards_t + (1.0 - dones_t) * gamma * next_q\n",
    "\n",
    "    loss = mse_loss(q_values, target_q)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # simple gradient clipping for stability\n",
    "    nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f42ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Training loop\n",
    "total_steps = 0\n",
    "loss_history = []\n",
    "reward_history = []\n",
    "eps_history = []\n",
    "\n",
    "# We'll keep a short moving average for printing\n",
    "print(\"Starting training...\")\n",
    "for episode in trange(1, num_episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0.0\n",
    "    episode_losses = []\n",
    "\n",
    "    for t in range(max_steps_per_episode):\n",
    "        epsilon = get_epsilon(total_steps)\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        done = bool(terminated or truncated)\n",
    "        replay_buffer.push(state, action, reward, next_state, float(done))\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        total_steps += 1\n",
    "\n",
    "        loss_val = optimize_model()\n",
    "        if loss_val is not None:\n",
    "            episode_losses.append(loss_val)\n",
    "            loss_history.append(loss_val)\n",
    "\n",
    "        # update target network periodically\n",
    "        if total_steps % target_update_every == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    reward_history.append(episode_reward)\n",
    "    eps_history.append(epsilon)\n",
    "\n",
    "    # average loss for this episode\n",
    "    avg_loss = np.mean(episode_losses) if episode_losses else 0.0\n",
    "\n",
    "    if episode % print_interval == 0 or episode == 1:\n",
    "        recent_rewards_avg = np.mean(reward_history[-print_interval:])\n",
    "        print(f\"Episode {episode:4d} | Reward: {episode_reward:8.2f} | AvgLast{print_interval}: {recent_rewards_avg:8.2f} | Epsilon: {epsilon:.3f} | AvgLoss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cece223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Save the trained model\n",
    "torch.save(policy_net.state_dict(), \"inventory_dqn_policy.pth\")\n",
    "print(\"Saved policy to inventory_dqn_policy.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Quick plots of reward and loss (useful to check convergence)\n",
    "# Simple smoothing helper\n",
    "def smooth(x, w=50):\n",
    "    if len(x) < w:\n",
    "        return x\n",
    "    return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(reward_history, alpha=0.4, label='episode reward')\n",
    "plt.plot(range(len(smooth(reward_history))), smooth(reward_history), label='smoothed')\n",
    "plt.title(\"Episode reward\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "if loss_history:\n",
    "    plt.plot(loss_history, alpha=0.6)\n",
    "    plt.title(\"Training MSE loss (per update)\")\n",
    "else:\n",
    "    plt.title(\"No loss data\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f191f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Test the trained policy (render the env to observe behavior)\n",
    "def test_policy(policy_model, episodes=10, render=True):\n",
    "    policy_model.eval()\n",
    "    results = []\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0.0\n",
    "        for t in range(env.max_steps):\n",
    "            with torch.no_grad():\n",
    "                st = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                action = int(policy_model(st).argmax().cpu().numpy())\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                print(f\"\\n=== Test Episode {ep+1} | Step {t+1} ===\")\n",
    "                env.render()\n",
    "                print(f\"Action chosen: {action}, Reward: {reward:.2f}\")\n",
    "            state = next_state\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        results.append(ep_reward)\n",
    "        print(f\"Test Episode {ep+1} finished | Total reward: {ep_reward:.2f}\")\n",
    "    return results\n",
    "\n",
    "# Load saved model (if you want to test a previously saved checkpoint)\n",
    "# policy_net.load_state_dict(torch.load(\"inventory_dqn_policy.pth\", map_location=device))\n",
    "\n",
    "test_results = test_policy(policy_net, episodes=5, render=True)\n",
    "print(\"Test episode rewards:\", test_results)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
