{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "# Import the required classes from the Inventory_env_class.py file that is provided\n",
    "from Inventory_env_class import InventoryManagementEnv, NormalizeObservation, ReplayBuffer, DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intializing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment instance - Notice we do not need to use gym.make here as I provided you the enviroment class\n",
    "env = InventoryManagementEnv()\n",
    "\n",
    "# Normalize the observation space for better training performance\n",
    "env = NormalizeObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"NormalizeObservation\" is a Gymnasium observation wrapper that scales each element of the observation vector by pre-defined factors. This helps with stabilizing training, especially when the different components of the state have diverse scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking out the observation and action spaces structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space which is continuous:\n",
      "Box(0.0, inf, (6,), float32)\n",
      "\n",
      "Action space which is discrete:\n",
      "Discrete(3)\n",
      "\n",
      "Observation space dimensions: 6\n",
      "Action space dimensions: 3\n"
     ]
    }
   ],
   "source": [
    "# Check the observation and action spaces\n",
    "print(f\"Observation space which is continuous:\\n{env.observation_space}\\n\")\n",
    "print(f\"Action space which is discrete:\\n{env.action_space}\\n\")\n",
    "\n",
    "# Check the dimensions of the observation space \n",
    "print(f\"Observation space dimensions: {env.observation_space.shape[0]}\")\n",
    "print(f\"Action space dimensions: {env.action_space.n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a test run to get a sense of how this enviroment operates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1 | Action taken: 1\n",
      "Step: 1\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 4.76, Product Price: 20.01\n",
      "Demand: 10.10, Cash: 994.94\n",
      "\n",
      "Episode 2 | Action taken: 1\n",
      "Step: 2\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 4.96, Product Price: 19.90\n",
      "Demand: 8.64, Cash: 990.18\n",
      "\n",
      "Episode 3 | Action taken: 0\n",
      "Step: 3\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 4.89, Product Price: 20.16\n",
      "Demand: 9.63, Cash: 990.18\n",
      "\n",
      "Episode 4 | Action taken: 0\n",
      "Step: 4\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 4.77, Product Price: 20.05\n",
      "Demand: 6.15, Cash: 990.18\n",
      "\n",
      "Episode 5 | Action taken: 1\n",
      "Step: 5\n",
      "Raw Inventory: 3.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.26, Product Price: 19.72\n",
      "Demand: 10.70, Cash: 985.41\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment to a start state\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Here we are just taking some random actions to see how the env works and what information rendering give us\n",
    "# We do this over 5 episodes\n",
    "for episode in range(5):\n",
    "    action = env.action_space.sample()  # Replace with agent policy\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"\\nEpisode {episode+1} | Action taken: {action}\")\n",
    "    env.render()\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>When developing the DQN solution you need a few things</u> ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Network architectures for Q and target net is provided in the CLass and therefore DQN is imported\n",
    "    - Remember the inputs to tese networks is the \"state\" (observation space = 6) and outputs are actions (3).\n",
    "    - So, we intialize them accordingly.\n",
    "    - Also the target network has to be the same as the Q net when initialized.\n",
    "\n",
    "Let us see how to do this.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU or CPU selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assign dimension values of state and actions\n",
    "input_dim = env.observation_space.shape[0]  # 6 dimensions\n",
    "output_dim = env.action_space.n             # 3 actions\n",
    "\n",
    "# Initialize networks\n",
    "Q_net = DQN(input_dim, output_dim).to(device)\n",
    "target_net = DQN(input_dim, output_dim).to(device)\n",
    "\n",
    "# Copy the weights from Q network to target network\n",
    "target_net.load_state_dict(Q_net.state_dict())\n",
    "\n",
    "# Put target network in \"no Training\" mode.\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Replay Buffer is a simple fixed-capacity buffer used to store experience transitions (state, action, reward, next state, done). These transitions are later sampled in batches for training the DQN.\n",
    "    - The \"ReplayBuffer\" class is already imported from the Class.\n",
    "    - To initialize it, we simply have to say what the buffer value should be. Typically 10000 but it can be higher or lower depending how much diversity you want in the data.\n",
    "\n",
    "Let us see how we can initialize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(capacity=10000)    # The value is your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At this point you have all the operators required to implement the training loop and this is the task of your Mini Project Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
